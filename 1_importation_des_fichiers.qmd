---
title: "Détecter des faux billets"
author: "CME"
format: pdf
editor: visual
---

## I- Contexte

::: {style="text-align: justify;"}
Vous êtes consultant Data Analyst dans une entreprise spécialisée dans la data. Votre entreprise a décroché une prestation en régie au sein de l’**Organisation nationale de lutte contre le faux-monnayage (ONCFM)**.

<img src="img/logo_oncfm.png" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/> 

Cette institution a pour objectif de mettre en place des méthodes d’identification des contrefaçons des billets en euros. Ils font donc appel à vous, spécialiste de la data, pour mettre en place une modélisation qui serait capable d’identifier automatiquement les vrais des faux billets. Et ce à partir simplement de certaines dimensions du billet et des éléments qui le composent.

Voici le [cahier des charges de l’ONCFM](doc/cahier_des_charges.pdf) ainsi que le [jeu de données](data_raw/billets.csv)

Le client souhaite que vous travailliez directement depuis ses locaux sous la responsabilité de Marie, responsable du projet d’analyse de données à l’ONCFM. Elle vous laissera une grande autonomie pendant votre mission, et vous demande simplement que vous lui présentiez vos résultats une fois la mission terminée. Elle souhaite voir quels sont les traitements et analyses que vous avez réalisés en amont, les différentes pistes explorées pour la construction de l’algorithme, ainsi que le modèle final retenu.

Après avoir lu en détail le cahier des charges, vous vous préparez à vous rendre à l’ONCFM pour prendre vos nouvelles fonctions. Vous notez tout de même un post-it qui se trouve sur le coin de votre bureau, laissé par un de vos collègues :
:::

## II- Importation des fichiers

Définir le miroir CRAN pour l'installation de packages R

```{r}
options(repos = c(CRAN = "https://cran.rstudio.com/"))
```

Lecture du fichier csv

```{r}
data <- read.csv("data_raw/billets.csv", sep =";")
```

## III- Résumé des datas

```{r}
summary(data)
```

Nous avons un dataframe de 7 colonnes et 1 500 lignes 1 colonne de type character 6 colonnes numériques

## IV- Description des variables

```{r}
if (!require(skimr)) install.packages("skimr")
library(skimr)
skim(data)
```

Nous avons 37 valeurs manquantes dans la colonne margin_low

On affiche les lignes ou "margin_low" est NA

```{r}
missing_margin_low <- data[is.na(data$margin_low), ]
print(missing_margin_low)
```

Quelles sont les valeurs de "is_genuine" ?

```{r}
valeur_unique <- unique(data$is_genuine)
print(valeur_unique)
```

Nous avons 2 valeurs uniques dans la colonne is_genuine =\> "True" ou "False"

Combien y a-t-il de chacune de ces valeurs ?

```{r}
valeur_compte <- table(data$is_genuine)
print(valeur_compte)
```

Il y a **500 valeurs False** et **1 000 valeurs True**

Visualisation sous forme de diagramme en secteurs

```{r}
# Calcul des pourcentages
pourcentage <- round(valeur_compte / sum(valeur_compte) * 100, 1)
pourcentage_labels <- paste(pourcentage, "%")

# Définir les marges du graphique (gauche, droite, bas, haut)
par(mar = c(1, 1, 1, 1))

# Définir le rapport d'aspect pour le graphique circulaire
par(pty = "s")

# Création du pie chart avec les labels (sans les valeurs)
pie(valeur_compte, 
    labels = names(valeur_compte), 
    main = "Distribution des valeurs de is_genuine", 
    col = c("purple", "skyblue"), 
    border = "white")

# Calcul des positions des étiquettes
positions <- cumsum(valeur_compte) - valeur_compte / 2
text(x = cos(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     y = sin(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     labels = pourcentage_labels, 
     cex = 1.2, col = "white")

```

### En résumé

**Nous avons un tableau regroupant les données de 1 500 billets**\

**1 colonne décrivant s'il s'agit de vrais ou faux billets :**\
- il y a 1 000 vrais billets 66.7% et 500 faux billets 33.3%\

**6 colonne décrivants le format de ces billets :**\
- diagonale\
- hauteur gauche\
- hauteur droite\
- marge basse\
- marge haute\
- longueur\

***37 billets*** n'ont pas l'information de la marge basse ("margin_low") dans le tableau.

Nous allons aggréger les données sur la colonnne is_genuine\
Afficher la valeur moyenne de chaque variable pour les lignes False et True afin de voir si nous pouvons observer des différences significatives sur une ou plusieurs variables.

```{r}
if (!require(dplyr)) install.packages("dplyr")

library(dplyr)
```

```{r}
resultats <- data %>%
  group_by(is_genuine) %>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE)))

print(resultats)
```

Il y a bien quelques différences de mesures entre les vrais et faux billets mais il est difficile d'utiliser ces moyennes pour notre objectif final qui sera de déterminer d'après les mesures de billets s'il s'agit de "vrais" ou de "faux".

## V- Remlacement des NaN de la colonne "margin_low"

Pour commencer il est nécessaire de traiter les valeurs manquantes de notre jeu de données.

Nous avons la possibilité de retirer ces 37 lignes ou bien de déterminer les valeurs manquantes de la colonne "margin_low" en fonction des autres variables présentes à l'aide d'une régréssion linéaire.

Regréssion linéaire multiple : Nous utilisons toutes les variables pour commencer et nous allons voir si elles sont toutes significatives pour la détermination de "margin_low". Nous fixons notre seuil de test à 5% Toutes les p-valeur inférieure à 0.05 seront donc considérée comme significatives. Si toutes les variables ne sont pas significatives, nous utiliserons la méthode backward et retirerons une à une toutes les variables non significatives.

Pour réaliser notre modèle de régression linéaire, nous allons travailler sur notre jeu de données sans NA

### 1- Régréssion linéaire

```{r}
# Suppression des lignes où 'margin_low' est NaN
data_without_na <- data[!is.na(data$margin_low), ]
```

```{r}
# Régression linéaire multiple pour prédire la variable margin_low
reg_lin <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na)
summary(reg_lin)
```

Les variables sont toutes significatives car leur p-value est inférieure à 5% (0.05) le niveau de test que nous souhaitons. Nous conservons donc toutes les variables pour la détermination de 'margin_low'

Nous aurions pu vouloir sélectionner automatiquement un modèle avec l'ensemble des variables avec le package stat

```{r}
reg_null <- lm(margin_low~1, data=data_without_na)
reg_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data)
```

```{r}
#Méthode backward
reg_backward <- step(reg_tot, direction="backward")
```

Le résultat nous montre que toutes les variables explicatives sont importantes dans le modèle pour prédire margin_low. la suppression de length à l'impact le plus négatif sur le modèle augmentant considérablement le RSS (somme des carrés des résidus) chaque suppression rend le modèle moins performant. Le modèle final sera le modèle complet

**Partons sur le modèle "reg_lin"** Nous pouvons tester le modèle sous forme de prédiction en renseignant toutes les variables pour déterminer une valeur à margin_low

### 2- Test du modèle

ici nous prenons une ligne au hasard, prenons les valeurs des 5 autres variables et nous allons prédire "margin_low" et contrôler que la valeur est proche de la valeur connue.

```{r}
data[90,]
```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev  <- predict(reg_lin, prev)
round(marg_low_prev,digits=2)
```

cela nous donnerait une valeur de 4.22 de "margin_low" d'après le modèle de régréssion linéaire C'est très proche des 4.24 réel.

Nous pourrions améliorer le modèle en calculant les mesures d'influence afin d'identifier et visualiser les observations atypiques. Cela permettrait de mieux comprendre leur impact sur les résultats de la régression linéaire.

### 3- Mesures d'influence

#### a- Les Leviers

Les Leviers mesurent l'influence des points de données sur les valeurs prédites, avec un focus sur l'échantillonnage de chaque observation.

Définition des paramètres

```{r}
#niveau de test
alpha <- 0.05
#nombre d'individus
n <- dim(data_without_na)[1]
#nombre de variables
p <- 6
```

Création d'un dataframe pour l'analyse

```{r}
#analyse sur les valeurs atypique ou influantes
analyses <- data.frame(obs=1:n)
```

Calcul des leviers et définition du seuil

```{r}
#calcul des leviers
analyses$levier <- hat(model.matrix(reg_lin))
seuil_levier <- 2*p/n
```

Visualisation des leviers

```{r}
library(ggplot2)
ggplot(data=analyses, aes(x=obs,y=levier))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_levier, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Leviers")+
  scale_x_continuous(breaks=seq(0,n,by=100))
```

Ensuite nous pourrions identifier les observations pour lesquelles les leviers sont supérieurs au seuil défini

```{r}
# crée un vecteur logique True = levier qui dépasse le seuil, sinon False
idl <- analyses$levier>seuil_levier
# affiche les premieres valeur du vecteur idl
head(idl)
# Affiche les valeurs de levier qui ont une valeur de vecteur True donc influente
analyses$levier[idl]
```

Nous pouvons afficher ces observations dans le dataframe initial (data_without_na)

```{r}
# Récupère les indices des lignes où idl est True
indices_influents <- which(idl)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents <- data_without_na[indices_influents, ]
# Affiche le DataFrame filtré
summary(data_influents)
```

Nous pouvons désormais observer si ces observations concernent plutôt des "vrais" ou "faux" billets.

```{r}
table(data_influents$is_genuine)
```

Avec ce seuil, On constate qu'il s'agit majoritairement de vrais billets.

#### b- Résidus Studentisés

les Résidus Studentisés mesurent combien une observation s'écarte des valeurs attendues en tenant compte de l'échantillon global.

Calculer les résidus studentisés et identifier les résidus extrêmes.

```{r}
analyses$rstudent <- rstudent(reg_lin)
seuil_rstudent <- qt(1-alpha/2,n-p-1)
```

```{r}
ggplot(data=analyses, aes(x=obs,y=rstudent))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=-seuil_rstudent, col="red")+
  geom_hline(yintercept=seuil_rstudent, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Résidus studentisés")+
  scale_x_continuous(breaks=seq(0,n,by=100))
```

Nous pouvons également afficher ces observations dans le dataframe initial (data_without_na)

```{r}
# Crée un vecteur logique True = résidu studentisé au-delà du seuil, sinon False
idr <- abs(analyses$rstudent) > seuil_rstudent
# Récupère les indices des lignes où idl est True
indices_influents_rstudent <- which(idr)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents_rstudent <- data_without_na[indices_influents_rstudent, ]
# Affiche le DataFrame filtré
summary(data_influents_rstudent)
```

et nous pouvons aussi observer si ces observations concernent plutôt des "vrais" ou "faux" billets.

```{r}
table(data_influents_rstudent$is_genuine)
```

Il s'agit majoritairement de faux billets dans ce cas.

#### c- Distance de Cook

La Distances de Cook est une combinaison des leviers et des résidus studentisés pour quantifier l'impact total d'une observation sur les coefficients du modèle.

Pour récupérer la distance de Cook

```{r}
influence<-influence.measures(reg_lin)
names(influence)
colnames(influence$infmat)
```

```{r}
analyses$dcook <- influence$infmat[,"cook.d"]
seuil_dcook <- 4/(n-p)
```

```{r}
seuil_dcook
```

```{r}
ggplot(data=analyses, aes(x=obs,y=dcook))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_dcook, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Distance de Cook")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

observons de quelles lignes il s'agit dans notre jeu de données

```{r}
# Crée un vecteur logique True = distance de Cook au-delà du seuil, sinon False
idc <- analyses$dcook > seuil_dcook
# Récupère les indices des lignes où idc est True
indices_influents_cook <- which(idc)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents_cook <- data_without_na[indices_influents_cook, ]
# Affiche le DataFrame filtré
summary(data_influents_cook)
```

Combien concernent des vrais et faux billets ?

```{r}
table(data_influents_cook$is_genuine)
```

Il s'agit aussi majoritairement de faux billets.

Dans un modèle de régression linéaire, il est important de vérifer deux aspects : - **la colinéarité** La colinéarité se produit lorsque deux ou plusieurs variables indépendantes dans un modèle de régression sont fortement corrélées, ce qui peut fausser les résultats et rendre difficile l'estimation précise des coefficients.

-   **l'homoscédascticité** L'homoscédasticité signifie que la variance des erreurs (ou résidus) est constante pour toutes les valeurs des variables explicatives. Cela est une hypothèse importante en régression linéaire car si cette hypothèse est violée (c'est-à-dire en cas d'hétéroscédasticité), les coefficients du modèle peuvent être biaisés et inefficaces.

### 4- Test de Non-colinéarité et d'homoscédasticité

Pour valider le modèle de régression linéaire et ces résultats, ces deux aspect doivent donc être respectés.

#### a- Colinéarité

Recherche de colinéarité (commande VIF)

```{r}
# Installation du package
install.packages("car")
# Charger le package
library(car)
vif(reg_lin)
```

Tous les VIF étant inférieur à 10, il ne semble pas y avoir de problème de colinéarité\
Les variables explicatives sont suffisamment indépendantes les unes des autres.

On peut également tester l'homoscédasticité, c'est à dire la constance de la variance des erreurs On peut donc réaliser le test de Breusch-Pagan

#### b- Homoscédasticité

```{R}
install.packages("lmtest")
library(lmtest)
bptest(reg_lin)
```

Hypothèse nulle (H₀) : Les résidus du modèle de régression sont homoscédastiques, c'est-à-dire que la variance des erreurs est constante.

Les résultats du test montrent une p-valeur extrêmement faible (p = 7,76e-16), bien inférieure au seuil de 0,05. Par conséquent, nous rejetons l'hypothèse nulle (H₀) et concluons à la présence d'hétéroscédasticité dans le modèle.

Cela implique que la variance des erreurs n'est pas constante, ce qui peut fausser les estimations des erreurs standards des coefficients de régression. Par conséquent, la validité des tests d'hypothèses et des intervalles de confiance est compromise, rendant le modèle présenté peu fiable dans sa forme actuelle.

Pour résoudre ce problème, nous allons appliquer des transformations aux variables, notamment en utilisant les méthodes de transformation logarithmique, au carré et inverse. Ces transformations visent à stabiliser la variance des résidus et à réduire l'hétéroscédasticité.

En effectuant ces transformations sur certaines variables explicatives et/ou sur la variable dépendante, nous espérons obtenir une distribution d'erreurs plus homogène, ce qui rendra les estimations des coefficients de régression plus robustes. Cette approche améliorera ainsi la fiabilité du modèle, tant en termes d'interprétation des coefficients que de validité des tests d'hypothèse.

### 5- Application de Transformations pour Stabiliser la Variance des Résidus

#### a- La transformation logarithmique

```{r}
# Régression linéaire multiple pour prédire la variable margin_low avec transformation lorgarithmique
reg_lin_log <- lm(log(margin_low)~log(diagonal)+log(height_left)+log(height_right)+log(margin_up)+log(length), data=data_without_na)
summary(reg_lin_log)
```

Reprenons la ligne du test de prédiction de "margin_low" et contrôlons la valeur obtenue.

```{r}
data[90,]
```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_log  <- predict(reg_lin_log, prev)
marg_low_prev <- exp(marg_low_prev_log)
round(marg_low_prev,digits=2)
```

Nous pouvons refaire le test d'homoscédasticité

```{r}
bptest(reg_lin_log)
```

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

P-value : La p-value obtenue est 4.652e-06, ce qui est extrêmement inférieure au seuil de significativité habituel de 0.05.

Étant donné que la p-value est bien inférieure à 0.05, nous pouvons rejeter l'hypothèse nulle (H0). Cela indique qu'il reste des preuves significatives d'hétéroscédasticité dans le modèle.

#### b- La transformation quadratique (Racine Carré)

```{r}
reg_lin_sqrt <- lm(sqrt(margin_low) ~ sqrt(diagonal) + sqrt(height_left) + sqrt(height_right) + sqrt(margin_up) + sqrt(length), data=data_without_na)
summary(reg_lin_sqrt)

```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_sqrt  <- predict(reg_lin_sqrt, prev)
marg_low_prev <- marg_low_prev_sqrt^2
round(marg_low_prev,digits=2)
```

```{r}
bptest(reg_lin_sqrt)
```

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

P-value : La p-value obtenue est 7.978e-11, ce qui est extrêmement inférieure au seuil de significativité habituel de 0.05.

Étant donné que la p-value est bien inférieure à 0.05, vous pouvez rejeter l'hypothèse nulle (H0). Cela indique qu'il y a des preuves significatives d'hétéroscédasticité dans les résidus de votre modèle, même après avoir appliqué la transformation racine carrée.

#### c- La transformation Inverse

```{r}
# Ajuster le modèle avec la transformation inverse sur la variable dépendante
reg_lin_inverse <- lm(I(1 / margin_low) ~ diagonal + height_left + height_right + margin_up + length, data = data_without_na)
summary(reg_lin_inverse)

```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_inverse  <- predict(reg_lin_inverse, prev)
marg_low_prev <- 1 /marg_low_prev_inverse
round(marg_low_prev,digits=2)
```

```{r}
bptest(reg_lin_inverse)
```

Hypothèse du Test Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

p-value : La p-value obtenue est de 0.4773.

Comme la p-value est bien supérieure au seuil de signification de 0.05, on ne rejete pas l'hypothèse nulle. Cela indique qu'il n'y a pas suffisamment de preuves pour conclure qu'il existe une hétéroscédasticité significative dans les résidus du modèle où seule la variable dépendante a été transformée par l'inverse.

Nous allons utiliser la régression linéaire, en appliquant une transformation inverse à la variable dépendante, afin de garantir la non-colinéarité et l'homoscédasticité des résidus. La non-colinéarité permet d'éviter la redondance entre les variables explicatives, assurant ainsi des estimations fiables des coefficients, tandis que l'homoscédasticité garantit que la variance des résidus reste constante, condition essentielle pour des tests statistiques valides et des intervalles de confiance précis.

## V- Remplacement des données manquantes

```{r}
# Prédire les valeurs manquantes
predicted_values_inverse <- predict(reg_lin_inverse, newdata = missing_margin_low)
predicted_values <- 1/predicted_values_inverse
```

```{r}
predicted_values
```

```{r}
# Étape 3: Remplacer les NA par les valeurs prédites
data$margin_low[is.na(data$margin_low)] <- predicted_values
```

```{r}
# Voir le résultat
summary(data$margin_low)
```

```{r}
missing_margin_low
```

On contrôle ici que les valeurs déterminées par le modèle ont bien remplacées les NA de notre jeu de donnée "data"

```{r}
indices <- rownames(missing_margin_low)
lignes_data <- data[indices, ]
print(lignes_data)

```

Puis on contrôle qu'il n'y a plus de valeur manquante dans "data"

```{r}
skim(data)
```

### Analyse des résidus de la régression logistique sur l'ensemble de "data"

Nous pouvons visualiser les résidus à l'échelle originale sur l'ensemble du dataframe

```{r}
predicted_values_inv_data <- predict(reg_lin_inverse, newdata = data)
predicted_values_original_data <- 1 / predicted_values_inv_data
residus_original <- data$margin_low - predicted_values_original_data

# Résumé des résidus dans l'échelle originale
summary(residus_original)

# Visualiser les résidus dans l'échelle originale
hist(residus_original, main = "Distribution des résidus (échelle originale)", xlab = "Résidus")

# Q-Q Plot des résidus pour vérifier la normalité
qqnorm(residus_original, main = "Q-Q Plot des Résidus")
qqline(residus_original, col = "red")
```

Les résidus montrent une distribution qui est globalement normale et symétrique autour de zéro, ce qui est un bon indicateur que les hypothèses de normalité sont respectées. L'histogramme des résidus présente une forme de cloche, et le Q-Q plot confirme que les résidus se rapprochent d'une distribution normale même si on peut observer un détachement en queue par rapport à la normale.

------------------------------------------------------------------------

## VI- ACP et Clustering

Nous allons désormais réaliser une analyse exploratoire des regroupements possible de billets à l'aide de la méthode des K-means qui est une méthode non supervisée et qui nous donnera un aperçu du regroupement naturel des billets en fonction de leurs mesures sans tenir compte de l'information "vrai" et "faux" de "is_genuine". Et nous allons pour commencer réaliser une ACP pour réduire la dimensionnalité et découvrir les corrélations entre les variables.

### 1- Recherche de valeurs aberrantes

#### a- Boxplot des variables

```{r}
# Boxplot pour chaque variable numérique
data_numeric <- data %>% select(where(is.numeric))

# Afficher boxplots
par(mfrow = c(2, 3))  # Adapter le nombre de graphiques selon le nombre de variables
for (col in colnames(data_numeric)) {
  boxplot(data_numeric[[col]], main = col, ylab = col)
}

```

#### b- Scatterplot des paires de variables

```{r}
if (!require(GGally)) install.packages("GGally")
library(GGally)
# Affichage des scatter plots avec corrélation, noms des variables, et sans valeurs des axes
plot <- ggpairs(data[, c("diagonal", "height_left", "height_right", "margin_low", "margin_up", "length")], 
                title = "Scatter plots avec corrélation",
                upper = list(continuous = wrap("cor", size = 5)),   # Corrélation dans la partie supérieure
                lower = list(continuous = "points"),               # Nuages de points dans la partie inférieure
                diag = list(continuous = wrap("barDiag", size = 5)),# Distribution diagonale
                axisLabels = "show")                               # Affiche les noms des variables

# Personnaliser pour retirer les valeurs numériques des axes mais garder les noms des variables
plot <- plot + theme(
  axis.text.x = element_blank(),  # Supprime les valeurs numériques de l'axe des abscisses
  axis.text.y = element_blank(),  # Supprime les valeurs numériques de l'axe des ordonnées
  axis.ticks = element_blank()    # Supprime les ticks des axes
)

# Afficher le graphique
print(plot)
```

La dispersion des points indique qu'il n'y a pas de corrélation évidente et linéaire entre 2 variables

#### c- Méthode IQR

```{r}
# Calculer l'IQR pour chaque colonne
iqr <- function(x) {
  q3 <- quantile(x, 0.75)
  q1 <- quantile(x, 0.25)
  q3 - q1
}

# Détecter les outliers pour chaque colonne
outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr_value <- iqr(x)
  lower_bound <- q1 - 1.5 * iqr_value
  upper_bound <- q3 + 1.5 * iqr_value
  x < lower_bound | x > upper_bound
}
```

```{r}
# Appliquer la fonction pour détecter les outliers
iqr_outliers_data <- data_numeric %>%
  mutate(across(everything(), ~ outliers(.)))

# Afficher les outliers
head(iqr_outliers_data)
```

```{r}
# Afficher les indices des outliers
iqr_outliers_indices <- which(rowSums(iqr_outliers_data) > 0)
print(iqr_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers
data_with_iqr_outliers <- data[iqr_outliers_indices, ]

print(data_with_iqr_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
iqr_count_by_is_genuine <- data_with_iqr_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(iqr_count_by_is_genuine)

```

Il y a 53 lignes d'outliers avec la méthode IQR\
40 lignes concernent des faux billets\
13 lignes concernent de vrais billets

#### d- Méthode Z_score

```{r}
if (!require(tidyverse)) install.packages("tidyverse")

# Charger les bibliothèques nécessaires
library(tidyverse)

# Sélectionner les colonnes numériques
data_numeric <- data %>% select(where(is.numeric))

# Calculer les scores Z
data_z <- scale(data_numeric)

# Vérifier les scores Z
head(data_z)

```

```{r}
summary(data_z)
```

```{r}
# Ajouter les scores Z au dataframe original en conservant la colonne de caractères
data_z_full <- cbind(Row_Index = 1:nrow(data), data %>% select(where(is.character)), as.data.frame(data_z))

# Afficher les premières lignes du nouveau dataframe
head(data_z_full)

```

```{r}
# Visualiser la distribution des scores Z pour la colonne 'diagonal'
ggplot(as.data.frame(data_z), aes(x=diagonal)) + 
  geom_histogram(binwidth=0.5, fill="blue", color="black") + 
  theme_minimal() +
  labs(title="Distribution des scores Z pour la colonne 'diagonal'")
```

On fixe à +3 et -3 la limite de z_score pour identifier les outliers

Puis on affiche les lignes pour lesquels il y a au moins une variables considérée comme outlier

```{r}
# Identifier les lignes avec des outliers (Z > 3 ou Z < -3)
z_outliers <- data_z_full %>%
  filter(apply(data_z, 1, function(row) any(row > 3 | row < -3)))

# Afficher les outliers
print(z_outliers)

```

```{r}
# Afficher les indices des outliers
z_outliers_indices <- z_outliers$Row_Index
print(z_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers avec le méthode du z-score
data_with_z_outliers <- data[z_outliers_indices, ]

print(data_with_z_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
z_count_by_is_genuine <- z_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(z_count_by_is_genuine)

```

Il y a 24 lignes d'outliers avec la méthode du Z_score\
17 lignes concernent des faux billets\
7 lignes concernent de vrais billets

### 2- Suppression des lignes d'outliers

Je vais commencer mon analyse en supprimant les outliers identifiés par la méthode du Z_Score\
Je vais donc supprimer 24 lignes du dataframe original "data"\
je le nommerai "data_without_ouliers_z"

```{r}
data_without_ouliers_z <- data[-z_outliers_indices, ]
summary(data_without_ouliers_z)
```

### 3- ACP

```{r}
if (!require(FactoMineR)) install.packages("FactoMineR")
if (!require(factoextra)) install.packages("factoextra")

library(FactoMineR)
library(factoextra)
```

```{r}
# Extraire les colonnes numériques
datanum_without_ouliers_z <- Filter(is.numeric, data_without_ouliers_z)
```

```{r}
# Réaliser l'ACP sans spécifier ncp
resultat_acp <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, graph = FALSE)

# Extraire les valeurs propres
valeurs_propres <- resultat_acp$eig[,1]
```

```{r}
# Calculer les pourcentages d'inertie expliquée par chaque composante
pourcentage_inertie <- 100 * valeurs_propres / sum(valeurs_propres)
# Calculer l'inertie cumulée
cumulative_inertia <- cumsum(pourcentage_inertie)
```

#### a- Eboulis des Composantes Principales

```{r}
# Créer un graphique avec un seul appel
par(mfrow = c(1, 1)) # Assurez-vous d'avoir une seule fenêtre graphique

# Créer un plot avec des barres pour les pourcentages d'inertie
barplot(pourcentage_inertie, main = "Éboulis des Composantes Principales",
        xlab = "Composante Principale", ylab = "Pourcentage d'Inertie",
        col = "lightblue", border = "blue", ylim = c(0, 100))

# Ajouter la courbe cumulée en superposition sur le même graphique
# Recréer le graphique avec la courbe cumulée en utilisant `plot` pour ne pas effacer les barres
par(new = TRUE) # Permet de superposer le nouveau graphique sur le précédent
plot(cumulative_inertia, type = "o", col = "red", pch = 19, 
     ylim = c(0, 100), xlab = "", ylab = "", axes = FALSE)

# Ajouter une légende
legend("topright", legend = c("Inertie Cumulée (%)"), col = "red", pch = 19, inset = c(0, 0.5))
```

On remarque avec l'ébouli qu'il est nécessaire d'avoir 5 composantes pour expliquer 90% de l'inertie des 6 variables. Il n'est pas très utile de réduire la dimensionalité dans notre si nous perdons en précision pour expliquer la variance.

Nous pouvons cependant réaliser l'ACP pour afficher le cercle des corrélations, notamment pour les 2 premières dimensions qui expliquent 60% de la variance totale.

```{r}
# on choisit 2 composantes principales qui représentent 60% de la variance
resultat_acp_optimal <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, ncp = 2, graph = FALSE)

```

```{r}
# Visualiser les individus et les variables pour les premières et dernières combinaisons de composantes
pairs_to_plot <- list(c(1, 2))  # Choisir les paires de composantes à afficher

# Visualiser les individus et les variables pour les différentes combinaisons de composantes
for (pair in pairs_to_plot) {
# Visualiser les individus
  print(fviz_pca_ind(resultat_acp_optimal, 
                     axes = pair, 
                     col.ind = "cos2", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                     repel = TRUE) +
        ggtitle(paste("Visualisation des Individus - Composantes", pair[1], "et", pair[2])))
  
  # Visualiser les variables
  print(fviz_pca_var(resultat_acp_optimal, 
                     axes = pair, 
                     col.var = "contrib", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")) +
        ggtitle(paste("Visualisation des Variables - Composantes", pair[1], "et", pair[2])))
}
```

Axes principaux :\
Dim1 (43.3%) : Cet axe explique 43.3% de la variance totale des données.\
Dim2 (17%) : Cet axe explique 17% de la variance totale des données.

Dim1 On observe :\
- une forte contribution positibe de la marge basse (margin_low)\
- une forte contribution négative de la longueur (length)\
une forte valeur de Dim1 indiquera un longueur de billet faible avec une marge basse élevée

Dim2 On observe :\
- une forte contribution positibe de la diagonale Une forte valeur de Dim2 indiquera une valeur de diagonale élevée

```{r}
# Extraire les scores des individus
scores <- resultat_acp_optimal$ind$coord

```

```{r}
head(scores)
```

### 4- Clustering avec K-means

#### a- Méthode du coude

```{r}
# Définir une plage de nombres de clusters à tester
k_values <- 1:10  # Tester de 1 à 10 clusters
wss <- numeric(length(k_values))  # Pour stocker la somme des carrés intra-cluster

# Calculer l'inertie pour chaque nombre de clusters
for (k in k_values) {
  kmeans_result <- kmeans(scores, centers = k, nstart = 25)  # nstart pour la reproductibilité
  wss[k] <- kmeans_result$tot.withinss
}

# Tracer la méthode du coude
plot(k_values, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Nombre de Clusters", ylab = "Somme des Carrés Intra-Cluster",
     main = "Méthode du Coude pour K-means Clustering")
```

La méthode du coude indique que l'ajout de clusters au-delà de 2 n'améliore plus significativement la dispersion intra-cluster, suggérant que 2 clusters est le nombre optimal.\
En utilisant 2 clusters avec K-means, on obtient une séparation efficace des données tout en évitant une complexité inutile.

#### b- Test avec 2 Clusters

```{r}
# Appliquer K-means clustering avec 2 clusters
kmeans_result <- kmeans(scores, centers = 2, nstart = 25)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters <- fviz_cluster(kmeans_result, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters <- plot_clusters + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 2 Clusters")

# Ajouter les centres des clusters au graphique
centroids <- kmeans_result$centers

plot_clusters <- plot_clusters + 
  geom_point(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2, label = rownames(centroids)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters)

```

Centroids :

```{r}
# Centres des clusters
print(kmeans_result$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters <- data.frame(data_without_ouliers_z, Cluster = kmeans_result$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters)

```

```{r}
count_values  <- data_with_clusters %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

La répartition des "False" et "True" de chaque cluster nous indique que : - Le cluster 1 correspond aux faux billets avec 96.7 % de faux billets - Le cluster 2 correspond aux vrais billets avec 98.8 % de vrais billets

Naturellement la méthode des K-means a séparé notre jeu de donnée en 2 groupes avec une efficacité globale ou accuracy de 98.10 %.

#### c- Test avec 3 Clusters

```{r}
# Appliquer K-means clustering avec 3 clusters
kmeans_result_2 <- kmeans(scores, centers = 3, nstart = 36)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters_2 <- fviz_cluster(kmeans_result_2, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters_2 <- plot_clusters_2 + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 3 Clusters")

# Ajouter les centres des clusters au graphique
centroids_2 <- kmeans_result_2$centers

plot_clusters_2 <- plot_clusters_2 + 
  geom_point(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2, label = rownames(centroids_2)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters_2)

```

```{r}
# Centres des clusters
print(kmeans_result_2$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result_2$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters_2 <- data.frame(data_without_ouliers_z, Cluster = kmeans_result_2$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters_2)

```

```{r}
count_values  <- data_with_clusters_2 %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

Ici nous avons : - cluster 1 avec 98.1 % de faux billets - cluster 2 avec 97.7 % de vrais billets - cluster 3 avec 99 % de vrais billets pour une efficacité globale de 98.3 %

Il y a une meilleure efficacité au global, le cluster 3 atteint jusqu'à 99% de vrais billets mais cela ne nous aide pas énormément dans notre objectif de détecter les faux billets.

Le but était de vérifier comment se comportait le modèle avec 3 clusters. L'idéal aurait été un clustering avec 100 % de vrai dans un cluster 100 % de faux dans un autre cluster et les douteux dans un 3ème cluster

## VII- Régression Logistique

### 1- Remplacement des valeurs de "is_genuine" par "0" et "1"

Nous allons commencer par remplacer "False" par "0" et "True" par "1" afin que "is_genuine" contienne des valeurs numériques binaires.

```{r}
# Convertir 'is_genuine' en binaire : True -> 1 et False -> 0
data$is_genuine <- ifelse(data$is_genuine == "True", 1, 0)

# Vérifiez les premiers enregistrements pour confirmer la transformation
head(data$is_genuine)

```

### 2- Régréssion logistique complète (toutes les variables)

```{r}
rl_model <- glm(is_genuine~diagonal+height_left+height_right+margin_low+margin_up+length,
family="binomial",data=data)
summary(rl_model)
```

Certaines des variables obtenues ont des p-valeurs qui sont inférieures au niveau de test de 5 %, ce qui nous indique qu'elles sont bien significatives. Certaines autres ne sont pas en dessous de ce seuil.

On peut donc passer sur une procédure de sélection en retirant les variables non significatives au fur et à mesure, mais nous pouvons aussi sélectionner automatiquement un modèle avec une commande telle que stepAIC , qui sélectionne de manière automatique un modèle en se basant sur le critère AIC.

### 3- Sélection du modèle (méthode AIC)

```{r}
library(MASS)
stepAIC(rl_model, direction = "both")
```

Critère AIC initial 97.22 Après avoir retiré "diagonal", le modèle à le meilleur AIC avec 95.23. Les autres étapes de suppression de variable augmentent l'AIC ce qui indique qu'elles sont importantes pour le modèle.

La procédure de sélection de modèle nous a permis de déterminer que la variable "diagonal" n'était pas nécessaire pour le modèle final. Les variables restantes ("height_right", "height_right", "margin_low", "margin_up", et "length") sont significatives et ont un impact important sur le modèle.\
Nous avons réussi à réduire l'AIC, indiquant que le modèle final est plus parcimonieux tout en conservant une bonne qualité d'ajustement.

#### a- Analyse des coefficients

3 variables ont une valeur absolue élevée ce qui indique qu'elles ont plus d'importance et d'influence dans la prédiction.

1 correspondant à la valeur "vrai" de is_genuine, un coefficient positif indique que si la valeur augmente, la probabilité de "vrai" billet augmente également inversement un coefficient négatif indique que plus la valeur de la variable augmente plus la probabilité d'un "faux" billet augmente également.

-   "margin_up" a un coeff négatif de -10.203 ce qui indique que plus la marge haute augmente, plus il y a de chance qu'il s'agisse d'un faux billet.\
-   "margin_low" avec un coeff négatif également de -5.910 a également une influence importante et indique que l'augmentation de la marge basse augmente les probabilités qu'il s'agisse d'un faux billet.\
-   "length" a un coeff positif de 5.988 qui indique à l'inverse que l'augmentation de la longueur du billet augmente la probabilité qu'il s'agisse d'un vrai billet.

#### b- Odds Ratios

Calculons les Odds Ratios pour indiquer comment les changements dans les variables indépendantes affectent la probabilité de vrais ou faux billets :

```{r}
rl_model_1 <- glm(is_genuine ~ height_left + height_right + margin_low + 
    margin_up + length,
family="binomial",data=data)
summary(rl_model_1)
```

```{r}
# Récupérer les coefficients du modèle final
coefficients <- coef(rl_model_1)

# Calculer les odds ratios en exponentiant les coefficients
odds_ratios <- exp(coefficients)

# Afficher les odds ratios sans exposant
formatted_odds_ratios <- formatC(odds_ratios, format = "f", digits = 5)


# Imprimer les odds ratios formatés
formatted_odds_ratios

```

height_left: 0.18964 Une augmentation d'une unité de height_left diminue les chances que le billet soit authentique d'environ 81% (puisque 0.18964\<1)

height_right: 0.09595 Une augmentation d'une unité de height_right diminue les chances que le billet soit authentique d'environ 90.5%.

margin_low: 0.00271 Une augmentation d'une unité de margin_low diminue les chances que le billet soit authentique d'environ 99.7%.

margin_up: 0.00004 Une augmentation d'une unité de margin_up diminue fortement les chances que le billet soit authentique.

length: 398.56936 Une augmentation d'une unité de length augmente de manière significative (par 398 fois) les chances que le billet soit authentique.

### 4- Evaluation des performances du modèle

#### a- Métrique de performance

##### Matrice de confusion

La matrice de confusion permet de comparer les prédictions du modèle avec les valeurs réelles.

```{r}
# Prédictions du modèle
predictions <- predict(rl_model_1, type = "response")

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
classes <- ifelse(predictions > 0.5, "True", "False")

# Créer la matrice de confusion
conf_matrix <- table(Predicted = classes, Actual = data$is_genuine)
print(conf_matrix)

```

Vrais Négatifs (VN) : 491\
Faux Négatifs (FN) : 4\
Faux Positifs (FP) : 9\
Vrais Positifs (VP) : 996

Pour une probabilité supérieur à 0.5, le billet est prédit comme bon et en dessous de 0.5 le billet est prédit comme faux. La matrice de confusion montre que le modèle a une très bonne performance en classant les cas True et False correctement avec peu d'erreurs avec un seuil à 0.5.

##### Mesures de performance

Précision : Proportion des prédictions de vrais positifs parmi toutes les prédictions de positifs. VP/(VP+FP)\
Sensibilité (Recall) : Proportion des vrais positifs parmi tous les cas positifs réels. VP/(VP+FN)\
Spécificité : Proportion des vrais négatifs parmi tous les cas négatifs réels. VN/(VN+FP)\
F1-Score : Moyenne harmonique de la précision et de la sensibilité. 2x((precision x recall)/(precision + recall))

```{r}
# Calculer les mesures de performance
precision_rl_model_1 <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])
recall_rl_model_1 <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[1, 2])
specificity_rl_model_1 <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[2, 1])
f1_score_rl_model_1 <- 2 * (precision_rl_model_1 * recall_rl_model_1) / (precision_rl_model_1 + recall_rl_model_1)
# Calculer Accuracy
accuracy_rl_model_1 <- (conf_matrix[1, 1] + conf_matrix[2, 2]) / sum(conf_matrix)      # (TP + TN) / Total

# Afficher les résultats
cat("Précision: ", precision_rl_model_1, "\n")
cat("Sensibilité (Recall): ", recall_rl_model_1, "\n")
cat("Spécificité: ", specificity_rl_model_1, "\n")
cat("F1-Score: ", f1_score_rl_model_1, "\n")
cat("Accuracy: ", accuracy_rl_model_1, "\n")
```

Précision (Precision) : 0.991 Cela signifie que parmi toutes les instances que le modèle a classées comme True, 99.1% sont effectivement True. C'est un excellent résultat, indiquant que le modèle est très précis dans ses prédictions positives.

Sensibilité (Recall) : 0.996 La sensibilité mesure la proportion des véritables True correctement identifiés par le modèle. Avec une sensibilité de 99.6%, votre modèle détecte presque tous les cas positifs.

Spécificité : 0.982 La spécificité mesure la proportion des véritables False correctement identifiés par le modèle. Avec une spécificité de 98.2%, le modèle est aussi très efficace pour identifier les cas négatifs.

F1-Score : 0.993 L'F1-Score est la moyenne harmonique de la précision et du rappel. Un F1-Score de 99.3% indique un excellent équilibre entre précision et rappel.

Mon objectif est d'obtenir un modèle avec une spécificité proche de 100% pour minimiser au maximum le risque de faux positif, il est donc important que tous les vrais négatifs soient identifier même si cela déteriore un peu la précision.

##### Courbe ROC et AUC

La courbe ROC (Receiver Operating Characteristic) est utile pour visualiser la performance du modèle à différents seuils de classification. L'AUC (Area Under the Curve) mesure la capacité globale du modèle à discriminer entre les classes.

```{r}
# Installer et charger le package pROC si ce n'est pas déjà fait
if (!require(pROC)) install.packages("pROC")
library(pROC)

# Calculer la courbe ROC
roc_curve <- roc(data$is_genuine, predictions)

# Tracer la courbe ROC
plot(roc_curve, main = "Courbe ROC")

# Calculer l'AUC
auc_value <- auc(roc_curve)
cat("AUC: ", auc_value, "\n")

```

AUC : 0.998894 L'AUC mesure la capacité globale du modèle à discriminer entre les classes. Une AUC proche de 1 indique que le modèle a une très bonne performance de classification, capable de distinguer presque parfaitement entre les classes True et False.

#### b- Validation croisée K-Fold :

La validation croisée K-Fold permet de tester la robustesse du modèle en le validant sur différentes sous-parties du dataset.

```{r}
# Installer et charger le package cvms si ce n'est pas déjà fait
if (!require(cvms)) install.packages("cvms")
if (!require(caret)) install.packages("caret")
library(cvms)
library(caret)

# Appliquer la validation croisée K-Fold
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(data$is_genuine, k = 10)

# Calculer les performances pour chaque fold
cv_results <- lapply(folds, function(fold) {
  train_data <- data[-fold, ]
  test_data <- data[fold, ]
  
  # Ajuster le modèle sur les données d'entraînement
  model <- glm(is_genuine ~ height_left + height_right + margin_low + margin_up + length,
                family = "binomial", data = train_data)
  
  # Prédictions sur les données de test
  test_predictions <- predict(model, newdata = test_data, type = "response")
  test_classes <- ifelse(test_predictions > 0.5, 1, 0)
  
  # Calculer la matrice de confusion et les mesures de performance
  confusion_test <- table(Predicted = test_classes, Actual = test_data$is_genuine)
  precision_test <- confusion_test[2, 2] / (confusion_test[2, 2] + confusion_test[1, 2])
  recall_test <- confusion_test[2, 2] / (confusion_test[2, 2] + confusion_test[2, 1])
  specificity_test <- confusion_test[1, 1] / (confusion_test[1, 1] + confusion_test[1, 2])
  f1_score_test <- 2 * (precision_test * recall_test) / (precision_test + recall_test)
  
  return(c(precision = precision_test, recall = recall_test, specificity = specificity_test, f1_score = f1_score_test))
})

# Moyenne des résultats de validation croisée
cv_results <- do.call(rbind, cv_results)
mean_cv_results <- colMeans(cv_results)
print(mean_cv_results)

```

Le lift est une mesure souvent utilisée pour évaluer l'amélioration de la prédiction par rapport à un modèle de base (par exemple, prédire la classe majoritaire). Les valeurs élevées du lift indiquent que le modèle offre des améliorations significatives par rapport à des méthodes naïves.

Conclusion Ces résultats montrent que le modèle de régression logistique est très performant avec des scores de précision, de rappel, de spécificité et de F1-Score très élevés, ainsi qu'une AUC presque parfaite. Cela suggère que le modèle est bien ajusté aux données et est capable de faire des prédictions avec une grande précision.

Prochaines Étapes Validation Externe : Si possible, testez le modèle sur un jeu de données complètement indépendant pour vérifier sa généralisation. Analyse des Résidus : Examinez les résidus pour identifier tout modèle non détecté ou problème potentiel. Exploration de Modèles Alternatifs : Bien que le modèle de régression logistique semble excellent, vous pouvez explorer d'autres modèles pour comparer les performances. Implémentation et Déploiement : Si le modèle répond à vos attentes, vous pouvez envisager de l'implémenter dans un environnement de production. Ces étapes vous aideront à assurer que votre modèle est non seulement performant mais aussi robuste et fiable dans des conditions réelles.

------------------------------------------------------------------------

### 5- Amélioration du modèle

Nous allons ajuster le seuil pour réduire la probabilité de classer un faux billet parmis les vrais, c'est à dire minimiser le risque de faux positifs.

```{r}
head(predictions)
```

#### a- Ajustement du seuil pour réduire le taux de faux positifs

La mesure de performance concernant la détection des faux billets est la spécificité dont le calcul est le suivant : VN/(VN+FP) Cela nous donne la proportion de vrais négatifs détectés parmi les négatifs.

##### Création d'une fonction pour calculer la spécificité

```{r}
# Fonction pour calculer la spécificité
calculate_specificity <- function(pred_probs, actual_classes, threshold) {
  predicted_classes <- ifelse(pred_probs > threshold, 1, 0)
  confusion <- table(predicted_classes, actual_classes)
  # Confusion peut avoir des noms différents selon les résultats, ajustez si nécessaire
  VN <- confusion["0", "0"]
  FP <- confusion["1", "0"]
  specificity <- VN / (VN + FP)
  return(specificity)
}

```

Plage de seuils à tester

```{r}
thresholds <- seq(0.5, 0.99, by = 0.01)
specificities <- sapply(thresholds, function(t) {
  calculate_specificity(predictions, data$is_genuine, t)
})
```

##### Recherche du seuil qui offre le maximum de spécificité

```{r}
best_threshold <- thresholds[which.max(specificities)]
best_specificity <- max(specificities)

cat("Meilleur seuil:", best_threshold, "\n")
cat("Spécificité à ce seuil:", best_specificity, "\n")
```

#### b- Métrique de performance

##### Matrice de confusion

On applique ce seuil optimal à notre modèle pour obtenir les prédictions finales

```{r}
final_predictions <- ifelse(predictions > best_threshold, "True", "False")
final_conf_matrix <- table(final_predictions, Actual = data$is_genuine)
print(final_conf_matrix)
```

Avec ce modèle et ce seuil de sensibilité, on obtient un résultat intéressant pour le nombre de faux positif. Il n'y a qu'un seul faux billet détecté comme positif sur les 1500 billets de notre jeu de donnée.

```{r}
taux_erreur <- 1/1500 * 100
print(paste("le taux de faux positif est de", format(taux_erreur, digits = 1, nsmall = 1), "% avec ce modèle et ce seuil"))
```

##### Mesures de performance

```{r}
# Calculer les mesures de performance
precision_rl_model_2 <- final_conf_matrix[2, 2] / (final_conf_matrix[2, 2] + final_conf_matrix[2, 1])
recall_rl_model_2 <- final_conf_matrix[2, 2] / (final_conf_matrix[2, 2] + final_conf_matrix[1, 2])
specificity_rl_model_2 <- final_conf_matrix[1, 1] / (final_conf_matrix[1, 1] + final_conf_matrix[2, 1])
f1_score_rl_model_2 <- 2 * (precision_rl_model_2 * recall_rl_model_2) / (precision_rl_model_2 + recall_rl_model_2)
# Calculer Accuracy
accuracy_rl_model_2 <- (final_conf_matrix[1, 1] + final_conf_matrix[2, 2]) / sum(final_conf_matrix)      # (TP + TN) / Total
# Afficher les résultats
cat("Précision: ", precision_rl_model_2, "\n")
cat("Sensibilité (Recall): ", recall_rl_model_2, "\n")
cat("Spécificité: ", specificity_rl_model_2, "\n")
cat("F1-Score: ", f1_score_rl_model_2, "\n")
cat("Accuracy: ", accuracy_rl_model_2, "\n")
```

```{r}
print(paste("L'éfficacité golobale (accuracy) du modèle avec un seuil de 0.5 était de", round(accuracy_rl_model_1*100, 2), "% tandis que pour le second avec le seuil qui réduit au maximum le risque de faux positif, l'éfficacité globale est de", round(accuracy_rl_model_2*100, 2), "%"))
```

En regardant les performances globales, on voit que nous avons effectivement amélioré la Spécificité, c'est à dire réduire le risque de faux positifs, passant d'une spécificité de 98.2% à 99.8%.\
99.8% des faux billets sont donc détectés dans notre jeu de données initial.\
Nous avons également augmenté la précision passant de 0.991 à 0.998, cela signifie que 99.8% des billets classés dans "True" sont réellement vrais.

Cependant nous avons une baisse de sensibilité passant de 0.996 à 0.952 ce qui signifie que seulement 95.2 % des vrais billets sont identifiés et classés comme tel, donc 4.8 % de vrais billets sont considérés comme faux par le modèle. et une baisse du F1_score passant de 0.993 à 0.97, c'est à dire un moins bon équilibre entre détection des faux et des vrais billets dans le global.

Nous allons tester d'autres méthodes de classification pour comparer les modèles et leur performance.

### 6- Test d'autres méthodes

#### a- Méthode RANDOM FOREST

```{r}
# Charger les packages nécessaires
if (!require(randomForest)) install.packages("randomForest")
if (!require(caret)) install.packages("caret")
library(randomForest)
library(caret)

# Préparer les données
features <- data[ , names(data) != "is_genuine"]
target <- as.factor(data$is_genuine)

# Re-niveler la variable cible pour que '1' (ou 'TRUE') soit la classe positive
target <- relevel(target, ref = "1")  # Assurez-vous que '1' est bien la classe positive

# Diviser les données en ensembles d'entraînement et de test
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Préparer les caractéristiques et la cible pour l'entraînement et le test
trainFeatures <- trainData[ , names(trainData) != "is_genuine"]
trainTarget <- as.factor(trainData$is_genuine)
trainTarget <- relevel(trainTarget, ref = "1")  # Re-niveler la variable cible dans les données d'entraînement

testFeatures <- testData[ , names(testData) != "is_genuine"]
testTarget <- as.factor(testData$is_genuine)
testTarget <- relevel(testTarget, ref = "1")  # Re-niveler aussi la variable cible dans les données de test

# Entraîner le modèle Random Forest pour classification
rf_model <- randomForest(x = trainFeatures, y = trainTarget, ntree = 100, importance = TRUE)
print(rf_model)

# Faire des prédictions sur l'ensemble de test
rf_predictions <- predict(rf_model, testFeatures)

# Convertir les prédictions en facteur avec les niveaux de testTarget
rf_predictions <- factor(rf_predictions, levels = levels(testTarget))

# Évaluer le modèle
rf_conf_matrix <- confusionMatrix(rf_predictions, testTarget)
print(rf_conf_matrix)

```

```{r}
rf_accuracy <- rf_conf_matrix$overall['Accuracy']
rf_specificity <- rf_conf_matrix$byClass['Specificity']
rf_sensitivity <- rf_conf_matrix$byClass['Sensitivity']
rf_precision <- rf_conf_matrix$byClass['Pos Pred Value']
rf_f1_score <- 2 * (rf_precision * rf_sensitivity) / (rf_precision + rf_sensitivity)
```

Utilisation des variables déterminées par StepAIC pour voir si cela améliore le modèle Randomforest

```{r}
# Préparer les données pour Random Forest en utilisant les variables sélectionnées
selected_vars <- c("height_left", "height_right", "margin_low", "margin_up", "length")

trainData_selected <- trainData[, c(selected_vars, "is_genuine")]
testData_selected <- testData[, c(selected_vars, "is_genuine")]

# Convertir 'is_genuine' en facteur et re-niveler pour que '1' soit la classe positive
trainData_selected$is_genuine <- relevel(as.factor(trainData_selected$is_genuine), ref = "1")
testData_selected$is_genuine <- relevel(as.factor(testData_selected$is_genuine), ref = "1")

library(randomForest)

# Construire le modèle Random Forest
rf_model_2 <- randomForest(is_genuine ~ ., data = trainData_selected, ntree = 100, importance = TRUE)

# Afficher les résultats du modèle
print(rf_model_2)

# Prédictions sur le jeu de test
predictions_2 <- predict(rf_model_2, newdata = testData_selected)

# Calculer la matrice de confusion et d'autres statistiques
library(caret)
rf2_conf_matrix <- confusionMatrix(predictions_2, testData_selected$is_genuine, positive = "1")
print(rf2_conf_matrix)
```

Les statistiques sont assez proche.

```{r}
rf2_accuracy <- rf2_conf_matrix$overall['Accuracy']
rf2_specificity <- rf2_conf_matrix$byClass['Specificity']
rf2_sensitivity <- rf2_conf_matrix$byClass['Sensitivity']
rf2_precision <- rf2_conf_matrix$byClass['Pos Pred Value']
rf2_f1_score <- 2 * (rf2_precision * rf2_sensitivity) / (rf2_precision + rf2_sensitivity)
```

#### b- Méthode de Réseau Neuronal

Utilisation de la librairie "reticulate" qui intègre python dans R Choix de l'environnement Conda utilisé (contient les installations spécifiques pour l'utilisation de Tensorflow et keras) Chargement des librairie

```{r}
library(reticulate)

# Utilisez le nouvel environnement Conda
use_condaenv("tf-env", required = TRUE)

library(keras)

# Vérifiez la configuration de Python
py_config()

```

```{r}
# Normalisation des variables continues
df_normalized <- data %>%
  mutate(across(c(diagonal, height_left, height_right, margin_low, margin_up, length), scale))

# Diviser les données en ensembles d'entraînement et de test
set.seed(52)  # Pour la reproductibilité
train_indices <- sample(1:nrow(df_normalized), 0.8 * nrow(df_normalized))
train_data <- df_normalized[train_indices, ]
test_data <- df_normalized[-train_indices, ]

# Préparer les entrées et sorties
x_train <- as.matrix(train_data %>% dplyr::select(everything(), -is_genuine))
y_train <- as.matrix(train_data$is_genuine)

x_test <- as.matrix(test_data %>% dplyr::select(everything(), -is_genuine))
y_test <- as.matrix(test_data$is_genuine)

```

Modèle 1 Réseau simple sans régularisation

```{r}
library(keras)

# Créer le modèle
model_neuronal_1 <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')
# Compiler le modèle
model_neuronal_1 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
# Évaluer le modèle
score1 <- model_neuronal_1 %>% evaluate(x_test, y_test, positive ="1")
print(score1)
```

Le modèle 1 est assez simple et n'inclut aucune forme de régularisation. Il pourrait avoir un certain degré de surajustement (overfitting) ou de sous-ajustement (underfitting), mais la précision de 70 % montre qu'il capture une partie des relations dans les données. La perte est relativement élevée, indiquant que le modèle pourrait être amélioré.

```{r}
# Faire des prédictions sur les données de test
predictions <- model_neuronal_1 %>% predict(x_test)

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
classes <- ifelse(predictions > 0.5, 1, 0)

# Créer la matrice de confusion
conf_matrix <- confusionMatrix(as.factor(classes), as.factor(y_test), positive ="1")

# Afficher la matrice de confusion
print(conf_matrix)

```

```{r}
# Extraire les métriques
accuracy_neurone_1 <- conf_matrix$overall['Accuracy']
specificity_neurone_1 <- conf_matrix$byClass['Specificity']
sensitivity_neurone_1 <- conf_matrix$byClass['Sensitivity']
f1_score_neurone_1 <- conf_matrix$byClass['F1']

# Afficher les résultats
cat("Accuracy:", accuracy_neurone_1, "\n")
cat("Specificity:", specificity_neurone_1, "\n")
cat("Sensitivity:", sensitivity_neurone_1, "\n")
cat("F1 Score:", f1_score_neurone_1, "\n")

```

Modèle 2 Ajout de dropout pour régularisation et taille de couche plus élevée (désactivation de certains neurones pour éviter le surapprentissage)

```{r}
# Créer le modèle
model_neuronal_2 <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')
# Compiler le modèle
model_neuronal_2 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)
# Évaluer le modèle
score2 <- model_neuronal_2 %>% evaluate(x_test, y_test)
print(score2)
```

Le modèle 2, bien que plus complexe et régularisé avec Dropout, a une performance plus faible que le modèle 1. Cela pourrait être dû à une sur-régularisation, où le modèle perd trop d'information lors du processus de régularisation. De plus, l'architecture plus grande pourrait nécessiter un entraînement plus long ou des ajustements supplémentaires.

```{r}
# Faire des prédictions sur les données de test
predictions <- model_neuronal_2 %>% predict(x_test)

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
classes <- ifelse(predictions > 0.5, 1, 0)

# Créer la matrice de confusion
conf_matrix <- confusionMatrix(as.factor(classes), as.factor(y_test), positive ="1")

# Afficher la matrice de confusion
print(conf_matrix)

```

```{r}
# Extraire les métriques
accuracy_neurone_2 <- conf_matrix$overall['Accuracy']
specificity_neurone_2 <- conf_matrix$byClass['Specificity']
sensitivity_neurone_2 <- conf_matrix$byClass['Sensitivity']
f1_score_neurone_2 <- conf_matrix$byClass['F1']

# Afficher les résultats
cat("Accuracy:", accuracy_neurone_2, "\n")
cat("Specificity:", specificity_neurone_2, "\n")
cat("Sensitivity:", sensitivity_neurone_2, "\n")
cat("F1 Score:", f1_score_neurone_2, "\n")

```

Modèle 3 simplifié sans dropout mais entrainé sur 20 époques

```{r}
# Créer un modèle plus simple
model_neuronal_simplified <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_simplified %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Entraîner le modèle
history <- model_neuronal_simplified %>% fit(
  x_train, y_train,
  epochs = 20,  # Augmenter le nombre d'époques si nécessaire
  batch_size = 32,
  validation_split = 0.2
)

# Évaluer le modèle
score_simplified <- model_neuronal_simplified %>% evaluate(x_test, y_test)
print(score_simplified)

```

Le modèle 3 est de loin le plus performant parmi les trois. La faible perte et la haute précision indiquent qu'il est très bien ajusté aux données. Cependant, il pourrait aussi y avoir un risque de surajustement (overfitting), car la performance sur l'ensemble de test est très proche de celle sur l'ensemble d'entraînement. L'absence de Dropout pourrait expliquer pourquoi le modèle performe aussi bien, mais cela signifie aussi que la généralisation à des données non vues pourrait poser problème si la complexité des données augmente.

```{r}
# Faire des prédictions sur les données de test
predictions <- model_neuronal_simplified %>% predict(x_test)

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
classes <- ifelse(predictions > 0.5, 1, 0)

# Créer la matrice de confusion
conf_matrix <- confusionMatrix(as.factor(classes), as.factor(y_test), positive ="1")

# Afficher la matrice de confusion
print(conf_matrix)

```

```{r}
# Extraire les métriques
accuracy_neurone_3 <- conf_matrix$overall['Accuracy']
specificity_neurone_3 <- conf_matrix$byClass['Specificity']
sensitivity_neurone_3 <- conf_matrix$byClass['Sensitivity']
f1_score_neurone_3 <- conf_matrix$byClass['F1']

# Afficher les résultats
cat("Accuracy:", accuracy_neurone_3, "\n")
cat("Specificity:", specificity_neurone_3, "\n")
cat("Sensitivity:", sensitivity_neurone_3, "\n")
cat("F1 Score:", f1_score_neurone_3, "\n")

```

Modèle 4 Dropout : Ajout d'un taux de 30 % de Dropout après chaque couche dense pour éviter que le modèle ne s'ajuste trop fortement aux données d'entraînement.

Régularisation L2 : Ajout d'une pénalisation L2 avec un coefficient de régularisation de 0.001 sur chaque couche dense pour limiter l'importance des poids.

Ajustement du taux d'apprentissage : Utilisation d'un plan de décroissance exponentielle du taux d'apprentissage pour affiner l'apprentissage avec le temps.

Early Stopping : Le modèle s'arrêtera automatiquement si la performance de validation ne s'améliore plus après 10 époques, et restaurera les meilleurs poids enregistrés.

Avantages :\
Meilleure généralisation : Grâce à la régularisation et au Dropout.\
Convergence optimale : Ajustement fin du taux d'apprentissage et contrôle du surapprentissage avec l'Early Stopping.

```{r}
# Stratégie d'ajustement du taux d'apprentissage (exponential decay)
learning_rate_schedule <- learning_rate_schedule_exponential_decay(
  initial_learning_rate = 0.001,
  decay_steps = 10000,
  decay_rate = 0.96,
  staircase = TRUE
)

# Optimizer avec le learning rate schedule
optimizer <- optimizer_adam(learning_rate = learning_rate_schedule)

# Créer le modèle amélioré avec Dropout et régularisation L2
model_neuronal_ameliore <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', 
              kernel_regularizer = regularizer_l2(0.001), input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.3) %>%  # Dropout de 30% après la première couche
  layer_dense(units = 32, activation = 'relu', 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.3) %>%  # Dropout de 30% après la deuxième couche
  layer_dense(units = 16, activation = 'relu', 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dense(units = 1, activation = 'sigmoid')  # Couche de sortie pour la classification binaire

# Compiler le modèle
model_neuronal_ameliore %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer,
  metrics = c('accuracy')
)

# Callback d'arrêt anticipé (Early Stopping)
early_stopping <- callback_early_stopping(
  monitor = "val_loss",  # Surveiller la perte sur l'ensemble de validation
  patience = 10,         # Nombre d'époques sans amélioration avant d'arrêter
  restore_best_weights = TRUE  # Restaurer les meilleurs poids
)

# Entraîner le modèle avec validation_split et arrêt anticipé
history <- model_neuronal_ameliore %>% fit(
  x_train, y_train,
  epochs = 50,  # Augmenter le nombre d'époques
  batch_size = 32,
  validation_split = 0.2,  # Utiliser 20% des données pour la validation
  callbacks = list(early_stopping)
)

# Évaluer le modèle sur les données de test
score_ameliore <- model_neuronal_ameliore %>% evaluate(x_test, y_test)
print(score_ameliore)


```

```{r}
# Faire des prédictions sur les données de test
predictions <- model_neuronal_ameliore %>% predict(x_test)

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
classes <- ifelse(predictions > 0.5, 1, 0)

# Créer la matrice de confusion
conf_matrix <- confusionMatrix(as.factor(classes), as.factor(y_test), positive ="1")

# Afficher la matrice de confusion
print(conf_matrix)

```

```{r}
# Extraire les métriques
accuracy_neurone_4 <- conf_matrix$overall['Accuracy']
specificity_neurone_4 <- conf_matrix$byClass['Specificity']
sensitivity_neurone_4 <- conf_matrix$byClass['Sensitivity']
f1_score_neurone_4 <- conf_matrix$byClass['F1']

# Afficher les résultats
cat("Accuracy:", accuracy_neurone_4, "\n")
cat("Specificity:", specificity_neurone_4, "\n")
cat("Sensitivity:", sensitivity_neurone_4, "\n")
cat("F1 Score:", f1_score_neurone_4, "\n")

```

COMPARAISON DES PERFORMANCES ENTRE LES MODELES

```{r}
# Installer et charger le package tibble pour une meilleure gestion des tableaux (si ce n'est pas déjà fait)
if (!require(tibble)) install.packages("tibble")
library(tibble)

# Créer un tableau de comparaison
results <- tibble(
  Model = c("Regression Logistique", "Regression Logistique opti", "Random Forest modele 1", "Random Forest modele 2",
            "Réseau neuronale 1", "Réseau neuronale 2", "Réseau neuronale 3", "Réseau neuronale 4"),
  Accuracy = c(accuracy_rl_model_1, accuracy_rl_model_2, rf_accuracy, rf2_accuracy,
                accuracy_neurone_1, accuracy_neurone_2, accuracy_neurone_3, accuracy_neurone_4),
  Specificity = c(specificity_rl_model_1, specificity_rl_model_2, rf_specificity, rf2_specificity,
                   specificity_neurone_1, specificity_neurone_2, specificity_neurone_3, specificity_neurone_4),
  Sensitivity = c(recall_rl_model_1, recall_rl_model_2, rf_sensitivity, rf2_sensitivity,
                   sensitivity_neurone_1, sensitivity_neurone_2, sensitivity_neurone_3, sensitivity_neurone_4),
  F1_Score = c(f1_score_rl_model_1, f1_score_rl_model_2, rf_f1_score, rf2_f1_score,
               f1_score_neurone_1, f1_score_neurone_2, f1_score_neurone_3, f1_score_neurone_4)
)

# Afficher le tableau
print(results)


```

Accuracy : Taux global de prédiction correcte. Specificity : Capacité à identifier les véritables négatifs. Sensitivity : Capacité à identifier les véritables positifs. F1 Score : Moyenne harmonique de la précision et du rappel, utile pour évaluer la performance dans des contextes déséquilibrés.

```{r}
library(ggplot2)

# Réorganiser les données pour le graphique
results_long <- results %>%
  pivot_longer(cols = c(Accuracy, Specificity, Sensitivity, F1_Score), names_to = "Metric", values_to = "Value")

# Créer un graphique comparatif
ggplot(results_long, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Comparison of Model Performances", x = "Model", y = "Value")

```

Sauvegarde du modèle à utiliser Modèle réseau neuronal 4

```{r}
# Sauvegarder le modèle neuronales (librairie keras)
model_neuronal_ameliore %>% save_model_hdf5("model_faux_billets.h5")

```

Modèle régréssion logistique opti il correspond à rl_model_1 avec application du seuil optimal

```{r}
# Sauvegarder le modèle et le seuil
saveRDS(rl_model_1, "modele_regression_logistique.rds")
saveRDS(best_threshold, "seuil_optimal.rds")
```
