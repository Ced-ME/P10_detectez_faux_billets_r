---
title: "Détecter des faux billets"
author: "CME"
format: html
editor: visual
---

## I- Contexte

Vous êtes consultant Data Analyst dans une entreprise spécialisée dans la data. Votre entreprise a décroché une prestation en régie au sein de l’**Organisation nationale de lutte contre le faux-monnayage (ONCFM)**.

![](img/logo_oncfm.png)

Cette institution a pour objectif de mettre en place des méthodes d’identification des contrefaçons des billets en euros. Ils font donc appel à vous, spécialiste de la data, pour mettre en place une modélisation qui serait capable d’identifier automatiquement les vrais des faux billets. Et ce à partir simplement de certaines dimensions du billet et des éléments qui le composent.

Voici le [cahier des charges de l’ONCFM](doc/cahier_des_charges.pdf) ainsi que le [jeu de données](data_raw/billets.csv)

Le client souhaite que vous travailliez directement depuis ses locaux sous la responsabilité de Marie, responsable du projet d’analyse de données à l’ONCFM. Elle vous laissera une grande autonomie pendant votre mission, et vous demande simplement que vous lui présentiez vos résultats une fois la mission terminée. Elle souhaite voir quels sont les traitements et analyses que vous avez réalisés en amont, les différentes pistes explorées pour la construction de l’algorithme, ainsi que le modèle final retenu.

Après avoir lu en détail le cahier des charges, vous vous préparez à vous rendre à l’ONCFM pour prendre vos nouvelles fonctions. Vous notez tout de même un post-it qui se trouve sur le coin de votre bureau, laissé par un de vos collègues :

## II- Importation des fichiers

```{r}
data <- read.csv("data_raw/billets.csv", sep =";")
```

## III- Résumé des datas

```{r}
summary(data)
```

Nous avons un dataframe de 7 colonnes et 1 500 lignes 1 colonne de type character 6 colonnes numériques

## IV- Description des variables

```{r}
if (!require(skimr)) install.packages("skimr")
library(skimr)

skim(data)

```

Nous avons 37 valeurs manquantes dans la colonne margin_low

```{r}
# Afficher les lignes où 'margin_low' est NA
missing_margin_low <- data[is.na(data$margin_low), ]
print(missing_margin_low)

```


```{r}
valeur_unique <- unique(data$is_genuine)
print(valeur_unique)
```

Nous avons 2 valeurs uniques dans la colonne is_genuine =\> True ou False

```{r}
valeur_compte <- table(data$is_genuine)
print(valeur_compte)
```

Il y a **500 valeurs False** et **1 000 valeurs True**

```{r}
# Calcul des pourcentages
pourcentage <- round(valeur_compte / sum(valeur_compte) * 100, 1)
pourcentage_labels <- paste(pourcentage, "%")

# Définir les marges du graphique (gauche, droite, bas, haut)
par(mar = c(1, 1, 1, 1))

# Définir le rapport d'aspect pour le graphique circulaire
par(pty = "s")

# Création du pie chart avec les labels (sans les valeurs)
pie(valeur_compte, 
    labels = names(valeur_compte), 
    main = "Distribution des valeurs de is_genuine", 
    col = c("purple", "skyblue"), 
    border = "white")

# Calcul des positions des étiquettes
positions <- cumsum(valeur_compte) - valeur_compte / 2
text(x = cos(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     y = sin(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     labels = pourcentage_labels, 
     cex = 1.2, col = "white")

```

### En résumé

Nous avons un tableau regroupant les données de 1 500 billets\

1 colonne décrivant s'il s'agit de vrais ou faux billets :\
- il y a 1 000 vrais billets 66.7% et 500 faux billets 33.3%\

6 colonne décrivants le format de ces billets :\
- diagonale\
- hauteur gauche\
- hauteur droite\
- marge basse\
- marge haute\
- longueur\

***37 billets*** n'ont pas l'information de la marge basse dans le tableau.

Nous allons aggréger les données sur la colonnne is_genuine\
Afficher la valeur moyenne de chaque variable pour les lignes False et True

```{r}
if (!require(dplyr)) install.packages("dplyr")

library(dplyr)
```

```{r}
resultats <- data %>%
  group_by(is_genuine) %>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE)))

print(resultats)
```

## V- Remlacement des NaN de la colonne "margin_low"

Regréssion linéaire multiple
Nous utilisons toutes les variables pour commencer et nous allons voir si elles sont toutes significatives pour la détermination de "margin_low".
Si ce n'est pas le cas nous utiliserons la méthode backward et retirerons une à une toutes les variables non significatives.

```{r}
# Supprimer les lignes où 'margin_low' est NaN
data_without_na <- data[!is.na(data$margin_low), ]

```

```{r}
reg_multi <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na)
summary(reg_multi)
```

Les variables sont toutes significatives car leur p-value est inférieure à 5% (0.05) le niveau de test que nous souhaitons.
Nous conservons donc toutes les variables pour la détermination de 'margin_low'

Nous pouvons tester le modèle sous forme de prédiction en renseignant toutes les variables pour déterminer une valeur à margin_low

```{r}
prev <- data.frame(diagonal=169, height_left=105, height_right=105, margin_up=3, length=115)
marg_low_prev <- predict(reg_multi, prev)
round(marg_low_prev,digits=2)
```

```{r}
#niveau de test
alpha <- 0.05
#nombre d'individus
n <- dim(data_without_na)[1]
#nombre de variables
p <- 6
```

```{r}
#analyse sur les valeurs atypique ou influantes
analyses <- data.frame(obs=1:n)
```

```{r}
#calcul des leviers
analyses$levier <- hat(model.matrix(reg_multi))
seuil_levier <- 2*p/n
```

```{r}
library(ggplot2)
ggplot(data=analyses, aes(x=obs,y=levier))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_levier, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Leviers")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

```{r}
#pour récupérer les numéros des observations pour lesquels les leviers sont supérieurs au seuil
idl <- analyses$levier>seuil_levier
head(idl)
analyses$levier[idl]
```

Calculer les résidus studentisés
```{r}
analyses$rstudent <- rstudent(reg_multi)
seuil_rstudent <- qt(1-alpha/2,n-p-1)
```

```{r}
ggplot(data=analyses, aes(x=obs,y=rstudent))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=-seuil_rstudent, col="red")+
  geom_hline(yintercept=seuil_rstudent, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Résidus studentisés")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

Pour récupérer la distance de Cook
```{r}
influence<-influence.measures(reg_multi)
names(influence)
colnames(influence$infmat)
```
```{r}
analyses$dcook <- influence$infmat[,"cook.d"]
seuil_dcook <- 4/(n-p)
```
```{r}
seuil_dcook
```



```{r}
ggplot(data=analyses, aes(x=obs,y=dcook))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_dcook, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Distance de Cook")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

Recherche de colinéarité (commande VIF)

```{r}
# Installer le package si ce n'est pas déjà fait
install.packages("car")

# Charger le package
library(car)

vif(reg_multi)
```

Tous les VIF étant inférieur à 10, il ne semble pas y avoir de problème de colinéarité

On peut également tester l'homoscédasticité, c'est à dire la constance de la variance des erreurs
On peut donc réaliser le test de Breusch-Pagan

```{R}
install.packages("lmtest")
library(lmtest)

bptest(reg_multi)
```

La p-valeur est inférieur à 0.05 et proche de 0
On rejete donc l'hypothèse nul H0
L'hypothèse nulle de ce test est que les erreurs ont une variance constante, c'est-à-dire que l'hétéroscédasticité n'est pas présente dans les résidus du modèle. En d'autres termes, l'hypothèse nulle affirme que les résidus sont homoscédastiques.

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante).
Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

La p-value obtenue est extrêmement petite (7.76e-16), bien en dessous du seuil de 0,05, qui est généralement utilisé pour les tests statistiques.

il y a des preuves significatives d'hétéroscédasticité dans le modèle de régression.  
En d'autres termes, la variance des erreurs n'est pas constante et cela peut biaiser les estimations de l'erreur standard des coefficients de régression, ce qui peut à son tour affecter la validité des tests d'hypothèses et des intervalles de confiance.

Autre point
Pour tester la normalité des résidus, nous pouvons utiliser le test de shapiro

```{r}
shapiro.test(reg_multi$residuals)
```

L'hypothèse de normalité est remise en cause car la p-value est inférieure à 0.05 et proche de 0


Nous aurions pu vouloir sélectionner automatiquement un modèle avec l'ensemble des variables
avec le package stat


```{r}
reg_null <- lm(margin_low~1, data=data_without_na)
reg_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data)
```

```{r}
#Méthode backward
reg_backward <- step(reg_tot, direction="backward")
```

Le résultat nous montre que toutes les variables explicatives sont importantes dans le modèle pour prédire margin_low.
la suppression de length à l'impact le plus négatif sur le modèle augmentant considérablement le RSS (somme des carrés des résidus)
chaque suppression rend le modèle moins performant.
Le modèle final sera le modèle complet






















ANCIENNE PARTIE A REVOIR

## V- Remplacement des données manquantes


```{r}
# Prédire les valeurs manquantes
predicted_values <- predict(reg_multi, newdata = missing_margin_low)
```

```{r}
predicted_values
```

```{r}
# Étape 3: Remplacer les NA par les valeurs prédites
data$margin_low[is.na(data$margin_low)] <- predicted_values
```

```{r}
# Voir le résultat
summary(data$margin_low)
```

```{r}
skim(data)
```

```{r}
# Obtenir les résidus du modèle
residus <- residuals(reg_multi)

# Résumé des résidus
summary(residus)

# Visualiser les résidus
hist(residus, main="Distribution des résidus", xlab="Résidus")

```



## VI- ACP et Clustering

### 1- Recherche de valeurs aberrantes

```{r}
# Boxplot pour chaque variable numérique
data_numeric <- data %>% select(where(is.numeric))

# Afficher boxplots
par(mfrow = c(2, 3))  # Adapter le nombre de graphiques selon le nombre de variables
for (col in colnames(data_numeric)) {
  boxplot(data_numeric[[col]], main = col, ylab = col)
}

```

```{r}
# Scatter plot entre deux variables
plot(data$diagonal, data$height_left, main = "Diagonale vs. Hauteur gauche", xlab = "Diagonale", ylab = "Hauteur gauche")

```

#### a- Méthode IQR

```{r}
# Calculer l'IQR pour chaque colonne
iqr <- function(x) {
  q3 <- quantile(x, 0.75)
  q1 <- quantile(x, 0.25)
  q3 - q1
}

# Détecter les outliers pour chaque colonne
outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr_value <- iqr(x)
  lower_bound <- q1 - 1.5 * iqr_value
  upper_bound <- q3 + 1.5 * iqr_value
  x < lower_bound | x > upper_bound
}
```

```{r}
# Appliquer la fonction pour détecter les outliers
iqr_outliers_data <- data_numeric %>%
  mutate(across(everything(), ~ outliers(.)))

# Afficher les outliers
head(iqr_outliers_data)
```

```{r}
# Afficher les indices des outliers
iqr_outliers_indices <- which(rowSums(iqr_outliers_data) > 0)
print(iqr_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers
data_with_iqr_outliers <- data[iqr_outliers_indices, ]

print(data_with_iqr_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
iqr_count_by_is_genuine <- data_with_iqr_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(iqr_count_by_is_genuine)

```

Il y a 53 lignes d'outliers avec la méthode IQR\
40 lignes concernent des faux billets\
13 lignes concernent de vrais billets

#### b- Méthode Z_score

```{r}
if (!require(tidyverse)) install.packages("tidyverse")

# Charger les bibliothèques nécessaires
library(tidyverse)

# Sélectionner les colonnes numériques
data_numeric <- data %>% select(where(is.numeric))

# Calculer les scores Z
data_z <- scale(data_numeric)

# Vérifier les scores Z
head(data_z)

```

```{r}
summary(data_z)
```

```{r}
# Ajouter les scores Z au dataframe original en conservant la colonne de caractères
data_z_full <- cbind(Row_Index = 1:nrow(data), data %>% select(where(is.character)), as.data.frame(data_z))

# Afficher les premières lignes du nouveau dataframe
head(data_z_full)

```

```{r}
# Visualiser la distribution des scores Z pour la colonne 'diagonal'
ggplot(as.data.frame(data_z), aes(x=diagonal)) + 
  geom_histogram(binwidth=0.5, fill="blue", color="black") + 
  theme_minimal() +
  labs(title="Distribution des scores Z pour la colonne 'diagonal'")
```

On fixe à +3 et -3 la limite de z_score pour identifier les outliers

```{r}
# Identifier les lignes avec des outliers (Z > 3 ou Z < -3)
z_outliers <- data_z_full %>%
  filter(apply(data_z, 1, function(row) any(row > 3 | row < -3)))

# Afficher les outliers
print(z_outliers)

```
```{r}
data_bis <- data
# Ajouter une colonne Row_index basée sur l'index de ligne
data_bis$Row_Index <- 1:nrow(data_bis)

```


```{r}
data_bis
```

```{r}
# Fusionner z_outliers avec la colonne is_genuine de data en utilisant Row_index
z_outliers <- merge(z_outliers, data_bis[, c("Row_Index", "is_genuine")], by = "Row_Index")

z_outliers
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
z_count_by_is_genuine <- z_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(z_count_by_is_genuine)

```

```{r}
# Extraire les indices des outliers
z_outliers_indices <- z_outliers$Row_Index
z_outliers_indices
```

Il y a 24 lignes d'outliers avec la méthode du Z_score\
17 lignes concernent des faux billets\
7 lignes concernent de vrais billets

### 2- Suppression des lignes d'outliers

Je vais commencer mon analyse en supprimant les outliers identifiés par la méthode du Z_Score\
Je vais donc supprimer 24 lignes du dataframe original "data"\
je le nommerai "data_without_ouliers_z"

```{r}
data_without_ouliers_z <- data[-z_outliers_indices, ]
data_without_ouliers_z
```

### 3- ACP

```{r}
if (!require(FactoMineR)) install.packages("FactoMineR")
if (!require(factoextra)) install.packages("factoextra")

library(FactoMineR)
library(factoextra)
```

```{r}
# Extraire les colonnes numériques
datanum_without_ouliers_z <- Filter(is.numeric, data_without_ouliers_z)
```

```{r}
# Réaliser l'ACP sans spécifier ncp
resultat_acp <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, graph = FALSE)

# Extraire les valeurs propres
valeurs_propres <- resultat_acp$eig[,1]
```

```{r}
# Calculer les pourcentages d'inertie expliquée par chaque composante
pourcentage_inertie <- 100 * valeurs_propres / sum(valeurs_propres)
# Calculer l'inertie cumulée
cumulative_inertia <- cumsum(pourcentage_inertie)
```

```{r}
# Créer un graphique avec un seul appel
par(mfrow = c(1, 1)) # Assurez-vous d'avoir une seule fenêtre graphique

# Créer un plot avec des barres pour les pourcentages d'inertie
barplot(pourcentage_inertie, main = "Éboulis des Composantes Principales",
        xlab = "Composante Principale", ylab = "Pourcentage d'Inertie",
        col = "lightblue", border = "blue", ylim = c(0, 100))

# Ajouter la courbe cumulée en superposition sur le même graphique
# Recréer le graphique avec la courbe cumulée en utilisant `plot` pour ne pas effacer les barres
par(new = TRUE) # Permet de superposer le nouveau graphique sur le précédent
plot(cumulative_inertia, type = "o", col = "red", pch = 19, 
     ylim = c(0, 100), xlab = "", ylab = "", axes = FALSE)

# Ajouter une légende
legend("topright", legend = c("Inertie Cumulée (%)"), col = "red", pch = 19, inset = c(0, 0.5))
```

```{r}
# on choisit 4 composantes principales qui représentent 80% de la variance
resultat_acp_optimal <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, ncp = 4, graph = FALSE)

```

```{r}
# Visualiser les individus et les variables pour les premières et dernières combinaisons de composantes
pairs_to_plot <- list(c(1, 2), c(3, 4))  # Choisir les paires de composantes à afficher

# Visualiser les individus et les variables pour les différentes combinaisons de composantes
for (pair in pairs_to_plot) {
# Visualiser les individus
  print(fviz_pca_ind(resultat_acp_optimal, 
                     axes = pair, 
                     col.ind = "cos2", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                     repel = TRUE) +
        ggtitle(paste("Visualisation des Individus - Composantes", pair[1], "et", pair[2])))
  
  # Visualiser les variables
  print(fviz_pca_var(resultat_acp_optimal, 
                     axes = pair, 
                     col.var = "contrib", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")) +
        ggtitle(paste("Visualisation des Variables - Composantes", pair[1], "et", pair[2])))
}
```

Axes principaux :\
Dim1 (45.9%) : Cet axe explique 45.9% de la variance totale des données.\
Dim2 (14.6%) : Cet axe explique 14.6% de la variance totale des données.

Dim1 On observe :\
- une forte contribution positibe de la marge basse (margin_low)\
- une forte contribution négative de la longueur (length)\
une forte valeur de Dim1 indiquera un longueur de billet faible avec une marge basse élevée

Dim2 On observe :\
- une forte contribution positibe de la diagonale Une forte valeur de Dim2 indiquera une valeur de diagonale élevée

```{r}
# Extraire les scores des individus
scores <- resultat_acp_optimal$ind$coord

```

```{r}
head(scores)
```

### 4- Clustering

```{r}
# Définir une plage de nombres de clusters à tester
k_values <- 1:10  # Tester de 1 à 10 clusters
wss <- numeric(length(k_values))  # Pour stocker la somme des carrés intra-cluster

# Calculer l'inertie pour chaque nombre de clusters
for (k in k_values) {
  kmeans_result <- kmeans(scores, centers = k, nstart = 25)  # nstart pour la reproductibilité
  wss[k] <- kmeans_result$tot.withinss
}

# Tracer la méthode du coude
plot(k_values, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Nombre de Clusters", ylab = "Somme des Carrés Intra-Cluster",
     main = "Méthode du Coude pour K-means Clustering")
```

```{r}
# Appliquer K-means clustering avec 2 clusters
kmeans_result <- kmeans(scores, centers = 2, nstart = 23)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters <- fviz_cluster(kmeans_result, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal()) +
  ggtitle("Clustering K-means avec 2 Clusters")

# Ajouter les centres des clusters au graphique
centroids <- kmeans_result$centers

plot_clusters + 
  geom_point(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2, label = rownames(centroids)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres


```

```{r}
# Centres des clusters
print(kmeans_result$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters <- data.frame(data_without_ouliers_z, Cluster = kmeans_result$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters)

```

```{r}
count_values  <- data_with_clusters %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```
