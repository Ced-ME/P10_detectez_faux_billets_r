---
title: "Détecter des faux billets"
author: "CME"
format: pdf
editor: visual
---

## I- Contexte

Vous êtes consultant Data Analyst dans une entreprise spécialisée dans la data. Votre entreprise a décroché une prestation en régie au sein de l’**Organisation nationale de lutte contre le faux-monnayage (ONCFM)**.

![](img/logo_oncfm.png)

Cette institution a pour objectif de mettre en place des méthodes d’identification des contrefaçons des billets en euros. Ils font donc appel à vous, spécialiste de la data, pour mettre en place une modélisation qui serait capable d’identifier automatiquement les vrais des faux billets. Et ce à partir simplement de certaines dimensions du billet et des éléments qui le composent.

Voici le [cahier des charges de l’ONCFM](doc/cahier_des_charges.pdf) ainsi que le [jeu de données](data_raw/billets.csv)

Le client souhaite que vous travailliez directement depuis ses locaux sous la responsabilité de Marie, responsable du projet d’analyse de données à l’ONCFM. Elle vous laissera une grande autonomie pendant votre mission, et vous demande simplement que vous lui présentiez vos résultats une fois la mission terminée. Elle souhaite voir quels sont les traitements et analyses que vous avez réalisés en amont, les différentes pistes explorées pour la construction de l’algorithme, ainsi que le modèle final retenu.

Après avoir lu en détail le cahier des charges, vous vous préparez à vous rendre à l’ONCFM pour prendre vos nouvelles fonctions. Vous notez tout de même un post-it qui se trouve sur le coin de votre bureau, laissé par un de vos collègues :

## II- Importation des fichiers

Définir le miroir CRAN pour l'installation de packages R

```{r}
options(repos = c(CRAN = "https://cran.rstudio.com/"))
```

Lecture du fichier csv

```{r}
data <- read.csv("data_raw/billets.csv", sep =";")
```

## III- Résumé des datas

```{r}
summary(data)
```

Nous avons un dataframe de 7 colonnes et 1 500 lignes 1 colonne de type character 6 colonnes numériques

## IV- Description des variables

```{r}
if (!require(skimr)) install.packages("skimr")
library(skimr)
skim(data)
```

Nous avons 37 valeurs manquantes dans la colonne margin_low

On affiche les lignes ou "margin_low" est NA

```{r}
missing_margin_low <- data[is.na(data$margin_low), ]
print(missing_margin_low)
```

Quelles sont les valeurs de "is_genuine" ?

```{r}
valeur_unique <- unique(data$is_genuine)
print(valeur_unique)
```

Nous avons 2 valeurs uniques dans la colonne is_genuine =\> "True" ou "False"

Combien y a-t-il de chacune de ces valeurs ?

```{r}
valeur_compte <- table(data$is_genuine)
print(valeur_compte)
```

Il y a **500 valeurs False** et **1 000 valeurs True**

Visualisation sous forme de diagramme en secteurs

```{r}
# Calcul des pourcentages
pourcentage <- round(valeur_compte / sum(valeur_compte) * 100, 1)
pourcentage_labels <- paste(pourcentage, "%")

# Définir les marges du graphique (gauche, droite, bas, haut)
par(mar = c(1, 1, 1, 1))

# Définir le rapport d'aspect pour le graphique circulaire
par(pty = "s")

# Création du pie chart avec les labels (sans les valeurs)
pie(valeur_compte, 
    labels = names(valeur_compte), 
    main = "Distribution des valeurs de is_genuine", 
    col = c("purple", "skyblue"), 
    border = "white")

# Calcul des positions des étiquettes
positions <- cumsum(valeur_compte) - valeur_compte / 2
text(x = cos(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     y = sin(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     labels = pourcentage_labels, 
     cex = 1.2, col = "white")

```

### En résumé

**Nous avons un tableau regroupant les données de 1 500 billets**\

**1 colonne décrivant s'il s'agit de vrais ou faux billets :**\
- il y a 1 000 vrais billets 66.7% et 500 faux billets 33.3%\

**6 colonne décrivants le format de ces billets :**\
- diagonale\
- hauteur gauche\
- hauteur droite\
- marge basse\
- marge haute\
- longueur\

***37 billets*** n'ont pas l'information de la marge basse ("margin_low") dans le tableau.

Nous allons aggréger les données sur la colonnne is_genuine\
Afficher la valeur moyenne de chaque variable pour les lignes False et True afin de voir si nous pouvons observer des différences significatives sur une ou plusieurs variables.

```{r}
if (!require(dplyr)) install.packages("dplyr")

library(dplyr)
```

```{r}
resultats <- data %>%
  group_by(is_genuine) %>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE)))

print(resultats)
```

Il y a bien quelques différences de mesures entre les vrais et faux billets mais il est difficile d'utiliser ces moyennes pour notre objectif final qui sera de déterminer d'après les mesures de billets s'il s'agit de "vrais" ou de "faux".

## V- Remlacement des NaN de la colonne "margin_low"

Pour commencer il est nécessaire de traiter les valeurs manquantes de notre jeu de données.

Nous avons la possibilité de retirer ces 37 lignes ou bien de déterminer les valeurs manquantes de la colonne "margin_low" en fonction des autres variables présentes à l'aide d'une régréssion linéaire.

Regréssion linéaire multiple : Nous utilisons toutes les variables pour commencer et nous allons voir si elles sont toutes significatives pour la détermination de "margin_low". Nous fixons notre seuil de test à 5% Toutes les p-valeur inférieure à 0.05 seront donc considérée comme significatives. Si toutes les variables ne sont pas significatives, nous utiliserons la méthode backward et retirerons une à une toutes les variables non significatives.

Pour réaliser notre modèle de régression linéaire, nous allons travailler sur notre jeu de données sans NA

### 1- Régréssion linéaire

```{r}
# Suppression des lignes où 'margin_low' est NaN
data_without_na <- data[!is.na(data$margin_low), ]
```

```{r}
# Régression linéaire multiple pour prédire la variable margin_low
reg_lin <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na)
summary(reg_lin)
```

Les variables sont toutes significatives car leur p-value est inférieure à 5% (0.05) le niveau de test que nous souhaitons. Nous conservons donc toutes les variables pour la détermination de 'margin_low'

Nous aurions pu vouloir sélectionner automatiquement un modèle avec l'ensemble des variables avec le package stat


```{r}
reg_null <- lm(margin_low~1, data=data_without_na)
reg_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data)
```

```{r}
#Méthode backward
reg_backward <- step(reg_tot, direction="backward")
```

Le résultat nous montre que toutes les variables explicatives sont importantes dans le modèle pour prédire margin_low. la suppression de length à l'impact le plus négatif sur le modèle augmentant considérablement le RSS (somme des carrés des résidus) chaque suppression rend le modèle moins performant. Le modèle final sera le modèle complet

**Partons sur le modèle "reg_lin"** Nous pouvons tester le modèle sous forme de prédiction en renseignant toutes les variables pour déterminer une valeur à margin_low

### 2- Test du modèle

ici nous prenons une ligne au hasard, prenons les valeurs des 5 autres variables et nous allons prédire "margin_low" et contrôler que la valeur est proche de la valeur connue.

```{r}
data[90,]
```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev  <- predict(reg_lin, prev)
round(marg_low_prev,digits=2)
```

cela nous donnerait une valeur de 4.22 de "margin_low" d'après le modèle de régréssion linéaire C'est très proche des 4.24 réel.

Nous pourrions améliorer le modèle en calculant les mesures d'influence afin d'identifier et visualiser les observations atypiques. Cela permettrait de mieux comprendre leur impact sur les résultats de la régression linéaire.

### 3- Mesures d'influence

#### a- Les Leviers

Les Leviers mesurent l'influence des points de données sur les valeurs prédites, avec un focus sur l'échantillonnage de chaque observation.

Définition des paramètres

```{r}
#niveau de test
alpha <- 0.05
#nombre d'individus
n <- dim(data_without_na)[1]
#nombre de variables
p <- 6
```

Création d'un dataframe pour l'analyse

```{r}
#analyse sur les valeurs atypique ou influantes
analyses <- data.frame(obs=1:n)
```

Calcul des leviers et définition du seuil

```{r}
#calcul des leviers
analyses$levier <- hat(model.matrix(reg_lin))
seuil_levier <- 2*p/n
```

Visualisation des leviers

```{r}
library(ggplot2)
ggplot(data=analyses, aes(x=obs,y=levier))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_levier, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Leviers")+
  scale_x_continuous(breaks=seq(0,n,by=100))
```

Ensuite nous pourrions identifier les observations pour lesquelles les leviers sont supérieurs au seuil défini

```{r}
# crée un vecteur logique True = levier qui dépasse le seuil, sinon False
idl <- analyses$levier>seuil_levier
# affiche les premieres valeur du vecteur idl
head(idl)
# Affiche les valeurs de levier qui ont une valeur de vecteur True donc influente
analyses$levier[idl]
```

Nous pouvons afficher ces observations dans le dataframe initial (data_without_na)

```{r}
# Récupère les indices des lignes où idl est True
indices_influents <- which(idl)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents <- data_without_na[indices_influents, ]
# Affiche le DataFrame filtré
summary(data_influents)
```

Nous pouvons désormais observer si ces observations concernent plutôt des "vrais" ou "faux" billets.

```{r}
table(data_influents$is_genuine)
```

Avec ce seuil, On constate qu'il s'agit majoritairement de vrais billets.

#### b- Résidus Studentisés

les Résidus Studentisés mesurent combien une observation s'écarte des valeurs attendues en tenant compte de l'échantillon global.

Calculer les résidus studentisés et identifier les résidus extrêmes.

```{r}
analyses$rstudent <- rstudent(reg_lin)
seuil_rstudent <- qt(1-alpha/2,n-p-1)
```

```{r}
ggplot(data=analyses, aes(x=obs,y=rstudent))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=-seuil_rstudent, col="red")+
  geom_hline(yintercept=seuil_rstudent, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Résidus studentisés")+
  scale_x_continuous(breaks=seq(0,n,by=100))
```

Nous pouvons également afficher ces observations dans le dataframe initial (data_without_na)

```{r}
# Crée un vecteur logique True = résidu studentisé au-delà du seuil, sinon False
idr <- abs(analyses$rstudent) > seuil_rstudent
# Récupère les indices des lignes où idl est True
indices_influents_rstudent <- which(idr)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents_rstudent <- data_without_na[indices_influents_rstudent, ]
# Affiche le DataFrame filtré
summary(data_influents_rstudent)
```

et nous pouvons aussi observer si ces observations concernent plutôt des "vrais" ou "faux" billets.

```{r}
table(data_influents_rstudent$is_genuine)
```

Il s'agit majoritairement de faux billets dans ce cas.

#### c- Distance de Cook

La Distances de Cook est une combinaison des leviers et des résidus studentisés pour quantifier l'impact total d'une observation sur les coefficients du modèle.

Pour récupérer la distance de Cook

```{r}
influence<-influence.measures(reg_lin)
names(influence)
colnames(influence$infmat)
```

```{r}
analyses$dcook <- influence$infmat[,"cook.d"]
seuil_dcook <- 4/(n-p)
```

```{r}
seuil_dcook
```

```{r}
ggplot(data=analyses, aes(x=obs,y=dcook))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_dcook, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Distance de Cook")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

observons de quelles lignes il s'agit dans notre jeu de données

```{r}
# Crée un vecteur logique True = distance de Cook au-delà du seuil, sinon False
idc <- analyses$dcook > seuil_dcook
# Récupère les indices des lignes où idc est True
indices_influents_cook <- which(idc)
# Affiche les lignes du DataFrame data_without_na avec ces indices
data_influents_cook <- data_without_na[indices_influents_cook, ]
# Affiche le DataFrame filtré
summary(data_influents_cook)
```

Combien concernent des vrais et faux billets ?

```{r}
table(data_influents_cook$is_genuine)
```
Il s'agit aussi majoritairement de faux billets.

Dans un modèle de régression linéaire, il est important de vérifer deux aspects : - **la colinéarité** La colinéarité se produit lorsque deux ou plusieurs variables indépendantes dans un modèle de régression sont fortement corrélées, ce qui peut fausser les résultats et rendre difficile l'estimation précise des coefficients.

-   **l'homoscédascticité** L'homoscédasticité signifie que la variance des erreurs (ou résidus) est constante pour toutes les valeurs des variables explicatives. Cela est une hypothèse importante en régression linéaire car si cette hypothèse est violée (c'est-à-dire en cas d'hétéroscédasticité), les coefficients du modèle peuvent être biaisés et inefficaces.

### 4- Test de Non-colinéarité et d'homoscédasticité

Pour valider le modèle de régression linéaire et ces résultats, ces deux aspect doivent donc être respectés.

#### a- Colinéarité

Recherche de colinéarité (commande VIF)

```{r}
# Installation du package
install.packages("car")
# Charger le package
library(car)
vif(reg_lin)
```

Tous les VIF étant inférieur à 10, il ne semble pas y avoir de problème de colinéarité\
Les variables explicatives sont suffisamment indépendantes les unes des autres.

On peut également tester l'homoscédasticité, c'est à dire la constance de la variance des erreurs On peut donc réaliser le test de Breusch-Pagan

#### b- Homoscédasticité

```{R}
install.packages("lmtest")
library(lmtest)
bptest(reg_lin)
```

Hypothèse nulle (H₀) : Les résidus du modèle de régression sont homoscédastiques, c'est-à-dire que la variance des erreurs est constante.

Les résultats du test montrent une p-valeur extrêmement faible (p = 7,76e-16), bien inférieure au seuil de 0,05. Par conséquent, nous rejetons l'hypothèse nulle (H₀) et concluons à la présence d'hétéroscédasticité dans le modèle.

Cela implique que la variance des erreurs n'est pas constante, ce qui peut fausser les estimations des erreurs standards des coefficients de régression. Par conséquent, la validité des tests d'hypothèses et des intervalles de confiance est compromise, rendant le modèle présenté peu fiable dans sa forme actuelle.

Pour résoudre ce problème, nous allons appliquer des transformations aux variables, notamment en utilisant les méthodes de transformation logarithmique, au carré et inverse. Ces transformations visent à stabiliser la variance des résidus et à réduire l'hétéroscédasticité.

En effectuant ces transformations sur certaines variables explicatives et/ou sur la variable dépendante, nous espérons obtenir une distribution d'erreurs plus homogène, ce qui rendra les estimations des coefficients de régression plus robustes. Cette approche améliorera ainsi la fiabilité du modèle, tant en termes d'interprétation des coefficients que de validité des tests d'hypothèse.

### 5- Application de Transformations pour Stabiliser la Variance des Résidus

#### a- La transformation logarithmique

```{r}
# Régression linéaire multiple pour prédire la variable margin_low avec transformation lorgarithmique
reg_lin_log <- lm(log(margin_low)~log(diagonal)+log(height_left)+log(height_right)+log(margin_up)+log(length), data=data_without_na)
summary(reg_lin_log)
```

Reprenons la ligne du test de prédiction de "margin_low" et contrôlons la valeur obtenue.

```{r}
data[90,]
```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_log  <- predict(reg_lin_log, prev)
marg_low_prev <- exp(marg_low_prev_log)
round(marg_low_prev,digits=2)
```

Nous pouvons refaire le test d'homoscédasticité

```{r}
bptest(reg_lin_log)
```

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

P-value : La p-value obtenue est 4.652e-06, ce qui est extrêmement inférieure au seuil de significativité habituel de 0.05.

Étant donné que la p-value est bien inférieure à 0.05, nous pouvons rejeter l'hypothèse nulle (H0). Cela indique qu'il reste des preuves significatives d'hétéroscédasticité dans le modèle.

#### b- La transformation quadratique (Racine Carré)

```{r}
reg_lin_sqrt <- lm(sqrt(margin_low) ~ sqrt(diagonal) + sqrt(height_left) + sqrt(height_right) + sqrt(margin_up) + sqrt(length), data=data_without_na)
summary(reg_lin_sqrt)

```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_sqrt  <- predict(reg_lin_sqrt, prev)
marg_low_prev <- marg_low_prev_sqrt^2
round(marg_low_prev,digits=2)
```

```{r}
bptest(reg_lin_sqrt)
```

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

P-value : La p-value obtenue est 7.978e-11, ce qui est extrêmement inférieure au seuil de significativité habituel de 0.05.

Étant donné que la p-value est bien inférieure à 0.05, vous pouvez rejeter l'hypothèse nulle (H0). Cela indique qu'il y a des preuves significatives d'hétéroscédasticité dans les résidus de votre modèle, même après avoir appliqué la transformation racine carrée.

#### c- La transformation Inverse

```{r}
# Ajuster le modèle avec la transformation inverse sur la variable dépendante
reg_lin_inverse <- lm(I(1 / margin_low) ~ diagonal + height_left + height_right + margin_up + length, data = data_without_na)
summary(reg_lin_inverse)

```

```{r}
prev <- data.frame(diagonal=171.94, height_left=103.49, height_right=103.82, margin_up=3.19, length=113.05)
marg_low_prev_inverse  <- predict(reg_lin_inverse, prev)
marg_low_prev <- 1 /marg_low_prev_inverse
round(marg_low_prev,digits=2)
```

```{r}
bptest(reg_lin_inverse)
```

Hypothèse du Test Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante). Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

p-value : La p-value obtenue est de 0.4773.

Comme la p-value est bien supérieure au seuil de signification de 0.05, on ne rejete pas l'hypothèse nulle. Cela indique qu'il n'y a pas suffisamment de preuves pour conclure qu'il existe une hétéroscédasticité significative dans les résidus du modèle où seule la variable dépendante a été transformée par l'inverse.

Avec cette méthode nous pouvons utiliser la régréssion linéaire pour prédire margin_low, la non-colinéarité et l'homoscédacité sont réspectée.

Nous pouvons également tester la régréssion linéaire multiple en séparant le jeu de données en 2, données "True" et données "False" et tester la performance.

#### d- Régression linéaire en séparant "True" et "False"

Nous pouvons également tester la régréssion linéaire multiple en séparant le jeu de données en 2, données "True" et données "False" et tester la performance.

##### Données "True"
```{r}
data_without_na_true <- data_without_na[data_without_na$is_genuine == "True", ]
data_without_na_true
```

```{r}
# Régression linéaire multiple pour prédire la variable margin_low
reg_lin_true <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na_true)
summary(reg_lin_true)
```

Seule la variable "margin_up" semble avoir une influence relative sur "margin_low" bien qu'elle ne soit pas concluante avec un niveau de signification de 0.05.
Les autres variables n'ont pas d'influence significative.

Méthode de sélection automatique des variables

```{r}
reg_true_null <- lm(margin_low~1, data=data_without_na_true)
reg_true_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na_true)
```

```{r}
#Méthode backward
reg_true_backward <- step(reg_true_tot, direction="backward")
```

Échantillonnage des Données pour Validation Croisée : Ensembles d'Entraînement et de Test
```{r}
set.seed(142)  # Pour rendre les résultats reproductibles

# Définir la proportion pour l'ensemble de test
test_proportion <- 0.2

# Calculer le nombre d'observations pour l'ensemble de test
total_rows <- nrow(data_without_na_true)
test_size <- round(test_proportion * total_rows)

# Créer un échantillon aléatoire d'indices pour l'ensemble de test
test_indices <- sample(1:total_rows, test_size)

# Créer les ensembles de test et d'entraînement
test_data <- data_without_na_true[test_indices, ]
train_data <- data_without_na_true[-test_indices, ]
```

```{r}
# Entraîner le modèle avec les données d'entraînement
reg_true_final <- lm(margin_low ~ margin_up, data = train_data)
summary(reg_true_final)
```

```{r}
# Prédictions avec le modèle sur les données de test
predictions <- predict(reg_true_final, test_data)
```

```{r}
# Calculer les valeurs réelles de margin_low dans l'ensemble de test
actual_values <- test_data$margin_low

# Calculer les erreurs
error <- abs(predictions - actual_values)
rmse <- sqrt(mean((predictions - actual_values)^2))
mae <- mean(error)

# Afficher les résultats
print(rmse)
print(mae)
```
RMSE indique la racine carrée de la moyenne des carrés des erreurs
MAE mesure la moyenne des valeurs absolues des erreurs de prédiction.

Nous pouvons visualiser les résultats à l'aide d'un nuage de points.
```{r}
library(ggplot2)

results <- data.frame(
  margin_up = test_data$margin_up,
  Predicted_margin_low = predictions,
  Actual_margin_low = actual_values
)

# Visualiser les résultats
ggplot(results, aes(x = margin_up)) +
  geom_point(aes(y = Predicted_margin_low, color = "Prédictions")) +
  geom_point(aes(y = Actual_margin_low, color = "Réels")) +
  labs(y = "margin_low", color = "Type") +
  theme_minimal()
```
En rouge nous voyons les prédictions issues de la régression linéaire et en bleu les valeurs réels.

##### Donéées "False"

```{r}
data_without_na_false <- data_without_na[data_without_na$is_genuine == "False", ]
data_without_na_false
```

```{r}
# Régression linéaire multiple pour prédire la variable margin_low
reg_multi_false <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na_false)
summary(reg_multi_false)
```

```{r}
reg_false_null <- lm(margin_low~1, data=data_without_na_false)
reg_false_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na_false)
```

```{r}
#Méthode backward
reg_false_backward <- step(reg_false_tot, direction="backward")
```

```{r}
set.seed(123)  # Pour rendre les résultats reproductibles

# Définir la proportion pour l'ensemble de test
test_proportion <- 0.2

# Calculer le nombre d'observations pour l'ensemble de test
total_rows <- nrow(data_without_na_false)
test_size <- round(test_proportion * total_rows)

# Créer un échantillon aléatoire d'indices pour l'ensemble de test
test_indices <- sample(1:total_rows, test_size)

# Créer les ensembles de test et d'entraînement
test_data <- data_without_na_false[test_indices, ]
train_data <- data_without_na_false[-test_indices, ]
```

```{r}
# Entraîner le modèle avec les données d'entraînement
reg_multi_final <- lm(margin_low ~ margin_up, data = train_data)
summary(reg_multi_final)
```

```{r}
# Prédictions avec le modèle sur les données de test
predictions <- predict(reg_multi_final, test_data)
```

```{r}
# Calculer les valeurs réelles de margin_low dans l'ensemble de test
actual_values <- test_data$margin_low

# Calculer les erreurs
error <- abs(predictions - actual_values)
rmse <- sqrt(mean((predictions - actual_values)^2))
mae <- mean(error)

# Afficher les résultats
print(rmse)
print(mae)
```

```{r}
library(ggplot2)

results <- data.frame(
  margin_up = test_data$margin_up,
  Predicted_margin_low = predictions,
  Actual_margin_low = actual_values
)

# Visualiser les résultats
ggplot(results, aes(x = margin_up)) +
  geom_point(aes(y = Predicted_margin_low, color = "Prédictions")) +
  geom_point(aes(y = Actual_margin_low, color = "Réels")) +
  labs(y = "margin_low", color = "Type") +
  theme_minimal()
```

Ce dernier modèle en séparant les data "True" et "False" semble avoir une capacité prédictive limitée. Ce ne sera pas ce modèle que nous conserverons.

Nous allons utiliser la régression linéaire, en appliquant une transformation inverse à la variable dépendante, afin de garantir la non-colinéarité et l'homoscédasticité des résidus. La non-colinéarité permet d'éviter la redondance entre les variables explicatives, assurant ainsi des estimations fiables des coefficients, tandis que l'homoscédasticité garantit que la variance des résidus reste constante, condition essentielle pour des tests statistiques valides et des intervalles de confiance précis.


## V- Remplacement des données manquantes

```{r}
# Prédire les valeurs manquantes
predicted_values_inverse <- predict(reg_lin_inverse, newdata = missing_margin_low)
predicted_values <- 1/predicted_values_inverse
```

```{r}
predicted_values
```

```{r}
# Étape 3: Remplacer les NA par les valeurs prédites
data$margin_low[is.na(data$margin_low)] <- predicted_values
```

```{r}
# Voir le résultat
summary(data$margin_low)
```

```{r}
missing_margin_low
```

On contrôle ici que les valeurs déterminées par le modèle ont bien remplacées les NA de notre jeu de donnée "data"

```{r}
indices <- rownames(missing_margin_low)
lignes_data <- data[indices, ]
print(lignes_data)

```

Puis on contrôle qu'il n'y a plus de valeur manquante dans "data"
```{r}
skim(data)
```


### Analyse des résidus de la régression logistique sur l'ensemble de "data"

Nous pouvons visualiser les résidus à l'échelle originale sur l'ensemble du dataframe
```{r}
predicted_values_inv_data <- predict(reg_lin_inverse, newdata = data)
predicted_values_original_data <- 1 / predicted_values_inv_data
residus_original <- data$margin_low - predicted_values_original_data

# Résumé des résidus dans l'échelle originale
summary(residus_original)

# Visualiser les résidus dans l'échelle originale
hist(residus_original, main = "Distribution des résidus (échelle originale)", xlab = "Résidus")

# Q-Q Plot des résidus pour vérifier la normalité
qqnorm(residus_original, main = "Q-Q Plot des Résidus")
qqline(residus_original, col = "red")
```

Les résidus montrent une distribution qui est globalement normale et symétrique autour de zéro, ce qui est un bon indicateur que les hypothèses de normalité sont respectées. L'histogramme des résidus présente une forme de cloche, et le Q-Q plot confirme que les résidus se rapprochent d'une distribution normale même si on peut observer un détachement en queue par rapport à la normale.


------------------------------------------------------------------------

## VI- ACP et Clustering

### 1- Recherche de valeurs aberrantes

```{r}
# Boxplot pour chaque variable numérique
data_numeric <- data %>% select(where(is.numeric))

# Afficher boxplots
par(mfrow = c(2, 3))  # Adapter le nombre de graphiques selon le nombre de variables
for (col in colnames(data_numeric)) {
  boxplot(data_numeric[[col]], main = col, ylab = col)
}

```

```{r}
# Scatter plot entre deux variables
plot(data$diagonal, data$height_left, main = "Diagonale vs. Hauteur gauche", xlab = "Diagonale", ylab = "Hauteur gauche")

```

#### a- Méthode IQR

```{r}
# Calculer l'IQR pour chaque colonne
iqr <- function(x) {
  q3 <- quantile(x, 0.75)
  q1 <- quantile(x, 0.25)
  q3 - q1
}

# Détecter les outliers pour chaque colonne
outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr_value <- iqr(x)
  lower_bound <- q1 - 1.5 * iqr_value
  upper_bound <- q3 + 1.5 * iqr_value
  x < lower_bound | x > upper_bound
}
```

```{r}
# Appliquer la fonction pour détecter les outliers
iqr_outliers_data <- data_numeric %>%
  mutate(across(everything(), ~ outliers(.)))

# Afficher les outliers
head(iqr_outliers_data)
```

```{r}
# Afficher les indices des outliers
iqr_outliers_indices <- which(rowSums(iqr_outliers_data) > 0)
print(iqr_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers
data_with_iqr_outliers <- data[iqr_outliers_indices, ]

print(data_with_iqr_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
iqr_count_by_is_genuine <- data_with_iqr_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(iqr_count_by_is_genuine)

```

Il y a 53 lignes d'outliers avec la méthode IQR\
40 lignes concernent des faux billets\
13 lignes concernent de vrais billets

#### b- Méthode Z_score

```{r}
if (!require(tidyverse)) install.packages("tidyverse")

# Charger les bibliothèques nécessaires
library(tidyverse)

# Sélectionner les colonnes numériques
data_numeric <- data %>% select(where(is.numeric))

# Calculer les scores Z
data_z <- scale(data_numeric)

# Vérifier les scores Z
head(data_z)

```

```{r}
summary(data_z)
```

```{r}
# Ajouter les scores Z au dataframe original en conservant la colonne de caractères
data_z_full <- cbind(Row_Index = 1:nrow(data), data %>% select(where(is.character)), as.data.frame(data_z))

# Afficher les premières lignes du nouveau dataframe
head(data_z_full)

```

```{r}
# Visualiser la distribution des scores Z pour la colonne 'diagonal'
ggplot(as.data.frame(data_z), aes(x=diagonal)) + 
  geom_histogram(binwidth=0.5, fill="blue", color="black") + 
  theme_minimal() +
  labs(title="Distribution des scores Z pour la colonne 'diagonal'")
```

On fixe à +3 et -3 la limite de z_score pour identifier les outliers

Puis on affiche les lignes pour lesquels il y a au moins une variables considérée comme outlier

```{r}
# Identifier les lignes avec des outliers (Z > 3 ou Z < -3)
z_outliers <- data_z_full %>%
  filter(apply(data_z, 1, function(row) any(row > 3 | row < -3)))

# Afficher les outliers
print(z_outliers)

```

```{r}
# Afficher les indices des outliers
z_outliers_indices <- z_outliers$Row_Index
print(z_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers avec le méthode du z-score
data_with_z_outliers <- data[z_outliers_indices, ]

print(data_with_z_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
z_count_by_is_genuine <- z_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(z_count_by_is_genuine)

```

Il y a 24 lignes d'outliers avec la méthode du Z_score\
17 lignes concernent des faux billets\
7 lignes concernent de vrais billets

### 2- Suppression des lignes d'outliers

Je vais commencer mon analyse en supprimant les outliers identifiés par la méthode du Z_Score\
Je vais donc supprimer 24 lignes du dataframe original "data"\
je le nommerai "data_without_ouliers_z"

```{r}
data_without_ouliers_z <- data[-z_outliers_indices, ]
data_without_ouliers_z
```

### 3- ACP

```{r}
if (!require(FactoMineR)) install.packages("FactoMineR")
if (!require(factoextra)) install.packages("factoextra")

library(FactoMineR)
library(factoextra)
```

```{r}
# Extraire les colonnes numériques
datanum_without_ouliers_z <- Filter(is.numeric, data_without_ouliers_z)
```

```{r}
# Réaliser l'ACP sans spécifier ncp
resultat_acp <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, graph = FALSE)

# Extraire les valeurs propres
valeurs_propres <- resultat_acp$eig[,1]
```

```{r}
# Calculer les pourcentages d'inertie expliquée par chaque composante
pourcentage_inertie <- 100 * valeurs_propres / sum(valeurs_propres)
# Calculer l'inertie cumulée
cumulative_inertia <- cumsum(pourcentage_inertie)
```

```{r}
# Créer un graphique avec un seul appel
par(mfrow = c(1, 1)) # Assurez-vous d'avoir une seule fenêtre graphique

# Créer un plot avec des barres pour les pourcentages d'inertie
barplot(pourcentage_inertie, main = "Éboulis des Composantes Principales",
        xlab = "Composante Principale", ylab = "Pourcentage d'Inertie",
        col = "lightblue", border = "blue", ylim = c(0, 100))

# Ajouter la courbe cumulée en superposition sur le même graphique
# Recréer le graphique avec la courbe cumulée en utilisant `plot` pour ne pas effacer les barres
par(new = TRUE) # Permet de superposer le nouveau graphique sur le précédent
plot(cumulative_inertia, type = "o", col = "red", pch = 19, 
     ylim = c(0, 100), xlab = "", ylab = "", axes = FALSE)

# Ajouter une légende
legend("topright", legend = c("Inertie Cumulée (%)"), col = "red", pch = 19, inset = c(0, 0.5))
```

```{r}
# on choisit 4 composantes principales qui représentent 80% de la variance
resultat_acp_optimal <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, ncp = 4, graph = FALSE)

```

```{r}
# Visualiser les individus et les variables pour les premières et dernières combinaisons de composantes
pairs_to_plot <- list(c(1, 2), c(3, 4))  # Choisir les paires de composantes à afficher

# Visualiser les individus et les variables pour les différentes combinaisons de composantes
for (pair in pairs_to_plot) {
# Visualiser les individus
  print(fviz_pca_ind(resultat_acp_optimal, 
                     axes = pair, 
                     col.ind = "cos2", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                     repel = TRUE) +
        ggtitle(paste("Visualisation des Individus - Composantes", pair[1], "et", pair[2])))
  
  # Visualiser les variables
  print(fviz_pca_var(resultat_acp_optimal, 
                     axes = pair, 
                     col.var = "contrib", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")) +
        ggtitle(paste("Visualisation des Variables - Composantes", pair[1], "et", pair[2])))
}
```

Axes principaux :\
Dim1 (43.3%) : Cet axe explique 43.3% de la variance totale des données.\
Dim2 (17%) : Cet axe explique 17% de la variance totale des données.

Dim1 On observe :\
- une forte contribution positibe de la marge basse (margin_low)\
- une forte contribution négative de la longueur (length)\
une forte valeur de Dim1 indiquera un longueur de billet faible avec une marge basse élevée

Dim2 On observe :\
- une forte contribution positibe de la diagonale Une forte valeur de Dim2 indiquera une valeur de diagonale élevée

```{r}
# Extraire les scores des individus
scores <- resultat_acp_optimal$ind$coord

```

```{r}
head(scores)
```

### 4- Clustering

```{r}
# Définir une plage de nombres de clusters à tester
k_values <- 1:10  # Tester de 1 à 10 clusters
wss <- numeric(length(k_values))  # Pour stocker la somme des carrés intra-cluster

# Calculer l'inertie pour chaque nombre de clusters
for (k in k_values) {
  kmeans_result <- kmeans(scores, centers = k, nstart = 25)  # nstart pour la reproductibilité
  wss[k] <- kmeans_result$tot.withinss
}

# Tracer la méthode du coude
plot(k_values, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Nombre de Clusters", ylab = "Somme des Carrés Intra-Cluster",
     main = "Méthode du Coude pour K-means Clustering")
```

TEST AVEC 2 CLUSTERS

```{r}
# Appliquer K-means clustering avec 2 clusters
kmeans_result <- kmeans(scores, centers = 2, nstart = 23)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters <- fviz_cluster(kmeans_result, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters <- plot_clusters + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 2 Clusters")

# Ajouter les centres des clusters au graphique
centroids <- kmeans_result$centers

plot_clusters <- plot_clusters + 
  geom_point(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2, label = rownames(centroids)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters)

```

```{r}
# Centres des clusters
print(kmeans_result$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters <- data.frame(data_without_ouliers_z, Cluster = kmeans_result$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters)

```

```{r}
count_values  <- data_with_clusters %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

TEST AVEC 3 CLUSTERS

```{r}
# Appliquer K-means clustering avec 3 clusters
kmeans_result_2 <- kmeans(scores, centers = 3, nstart = 33)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters_2 <- fviz_cluster(kmeans_result_2, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters_2 <- plot_clusters_2 + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 3 Clusters")

# Ajouter les centres des clusters au graphique
centroids_2 <- kmeans_result_2$centers

plot_clusters_2 <- plot_clusters_2 + 
  geom_point(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2, label = rownames(centroids_2)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters_2)

```

```{r}
# Centres des clusters
print(kmeans_result_2$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result_2$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters_2 <- data.frame(data_without_ouliers_z, Cluster = kmeans_result_2$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters_2)

```

```{r}
count_values  <- data_with_clusters_2 %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

Le but était de vérifier comment se comportait le modèle avec 3 clusters. L'idéal aurait été un clustering avec 100 % de vrai dans un cluster 100 % de faux dans un autre cluster et les douteux dans un 3ème cluster

## VII- Régression Logistique

### 1- Remplacement des valeurs de "is_genuine" par "0" et "1"

Nous allons commencer par remplacer "False" par "0" et "True" par "1" afin que "is_genuine" contienne des valeurs numériques binaires.

```{r}
# Convertir 'is_genuine' en binaire : True -> 1 et False -> 0
data$is_genuine <- ifelse(data$is_genuine == "True", 1, 0)

# Vérifiez les premiers enregistrements pour confirmer la transformation
head(data$is_genuine)

```

### 2- Régréssion logistique complète (toutes les variables)

```{r}
reg_log <- glm(is_genuine~diagonal+height_left+height_right+margin_low+margin_up+length,
family="binomial",data=data)
summary(reg_log)
```

Certaines des variables obtenues ont des p-valeurs qui sont inférieures au niveau de test de 5 %, ce qui nous indique qu'elles sont bien significatives. Certaines autres ne sont pas en dessous de ce seuil.

On peut donc passer sur une procédure de sélection en retirant les variables non significatives au fur et à mesure, mais nous pouvons aussi sélectionner automatiquement un modèle avec une commande telle que stepAIC , qui sélectionne de manière automatique un modèle en se basant sur le critère AIC.

### 3- Sélection du modèle (méthode AIC)

```{r}
library(MASS)
stepAIC(reg_log, direction = "both")
```

La procédure de sélection de modèle nous a permis de déterminer que les variables "diagonal" et "height_left" n'étaient pas nécessaires pour le modèle final. Les variables restantes ("height_right", "margin_low", "margin_up", et "length") sont significatives et ont un impact important sur le modèle.\
Nous avons réussi à réduire l'AIC, indiquant que le modèle final est plus parcimonieux tout en conservant une bonne qualité d'ajustement.

Analyse des coefficients : 3 variables ont une valeur absolue élevée ce qui indique qu'elles ont plus d'importance et d'influence dans la prédiction. - "margin_up" a un coeff négatif de -10.167 ce qui indique que plus la marge haute haute augmente, plus il y a de chance qu'il s'agisse d'un faux billet. - "margin_low" avec un coeff négatif également de -5.794 a également une influence importante et indique que l'augmentation de la marge basse augmente les probabilités qu'il s'agisse d'un faux billet. - "length" a un coeff positif de 5.920 qui indique à l'inverse que l'augmentation de la longueur du billet augmente la probabilité qu'il s'agisse d'un vrai billet.

Calculons les Odds Ratios pour indiquer comment les changements dans les variables indépendantes affectent la probabilité de vrais ou faux billets :

```{r}
reg_log_model <- glm(is_genuine ~ height_left + height_right + margin_low + 
    margin_up + length,
family="binomial",data=data)
summary(reg_log_model)
```

```{r}
# Récupérer les coefficients du modèle final
coefficients <- coef(reg_log_model)

# Calculer les odds ratios en exponentiant les coefficients
odds_ratios <- exp(coefficients)

# Afficher les odds ratios sans exposant
formatted_odds_ratios <- formatC(odds_ratios, format = "f", digits = 5)


# Imprimer les odds ratios formatés
formatted_odds_ratios

```

height_left: 0.17986 Une augmentation d'une unité de height_left diminue les chances que le billet soit authentique d'environ 82% (puisque 0.17986\<1)

height_right: 0.10402 Une augmentation d'une unité de height_right diminue les chances que le billet soit authentique d'environ 90%.

margin_low: 0.00305 Une augmentation d'une unité de margin_low diminue les chances que le billet soit authentique d'environ 99.7%.

margin_up: 0.00004 Une augmentation d'une unité de margin_up diminue fortement les chances que le billet soit authentique.

length: 372.5465 Une augmentation d'une unité de length augmente de manière significative (par 372 fois) les chances que le billet soit authentique.

### 4- Evaluation des performances du modèle

#### a- Métrique de performance

##### Matrice de confusion

La matrice de confusion permet de comparer les prédictions du modèle avec les valeurs réelles.

```{r}
# Prédictions du modèle
predic_reg_log <- predict(reg_log_model, type = "response")

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
predic_reg_log_classes <- ifelse(predic_reg_log > 0.5, "True", "False")

# Créer la matrice de confusion
confusion_reg_log <- table(Predicted = predic_reg_log_classes, Actual = data$is_genuine)
print(confusion_reg_log)

```

Vrais Négatifs (VN) : 491\
Faux Négatifs (FN) : 4\
Faux Positifs (FP) : 9\
Vrais Positifs (VP) : 996

La matrice de confusion montre que votre modèle a une très bonne performance en classant les cas True et False correctement avec peu d'erreurs.

##### Mesures de performance des positifs

Précision : Proportion des prédictions correctes parmi toutes les prédictions. VP/(VP+FP)\
Sensibilité (Recall) : Proportion des vrais positifs parmi tous les cas positifs réels. VP/(VP+FN)\
Spécificité : Proportion des vrais négatifs parmi tous les cas négatifs réels. VN/(VN+FP)\
F1-Score : Moyenne harmonique de la précision et de la sensibilité. 2x((precision x recall)/(precision + recall))

```{r}
# Calculer les mesures de performance
precision <- confusion_reg_log[2, 2] / (confusion_reg_log[2, 2] + confusion_reg_log[2, 1])
recall <- confusion_reg_log[2, 2] / (confusion_reg_log[2, 2] + confusion_reg_log[1, 2])
specificity <- confusion_reg_log[1, 1] / (confusion_reg_log[1, 1] + confusion_reg_log[2, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)
# Calculer Accuracy
model_regression_logistic_accuracy <- (confusion_reg_log[1, 1] + confusion_reg_log[2, 2]) / sum(confusion_reg_log)      # (TP + TN) / Total

# Afficher les résultats
cat("Précision: ", precision, "\n")
cat("Sensibilité (Recall): ", recall, "\n")
cat("Spécificité: ", specificity, "\n")
cat("F1-Score: ", f1_score, "\n")
cat("Accuracy: ", model_regression_logistic_accuracy, "\n")
```

Précision (Precision) : 0.991 Cela signifie que parmi toutes les instances que le modèle a classées comme True, 99.1% sont effectivement True. C'est un excellent résultat, indiquant que le modèle est très précis dans ses prédictions positives.

Sensibilité (Recall) : 0.996 La sensibilité mesure la proportion des véritables True correctement identifiés par le modèle. Avec une sensibilité de 99.6%, votre modèle détecte presque tous les cas positifs.

Spécificité : 0.982 La spécificité mesure la proportion des véritables False correctement identifiés par le modèle. Avec une spécificité de 98.2%, le modèle est aussi très efficace pour identifier les cas négatifs.

F1-Score : 0.993 L'F1-Score est la moyenne harmonique de la précision et du rappel. Un F1-Score de 99.3% indique un excellent équilibre entre précision et rappel.

Mon objectif est d'obtenir un modèle avec une spécificité proche de 100% pour minimiser au maximum le risque de faux positif, il est donc important que tous les vrais négatifs soient identifier même si cela déteriore un peu la précision.

##### Courbe ROC et AUC

La courbe ROC (Receiver Operating Characteristic) est utile pour visualiser la performance du modèle à différents seuils de classification. L'AUC (Area Under the Curve) mesure la capacité globale du modèle à discriminer entre les classes.

```{r}
# Installer et charger le package pROC si ce n'est pas déjà fait
if (!require(pROC)) install.packages("pROC")
library(pROC)

# Calculer la courbe ROC
roc_curve <- roc(data$is_genuine, predic_reg_log)

# Tracer la courbe ROC
plot(roc_curve, main = "Courbe ROC")

# Calculer l'AUC
auc_value <- auc(roc_curve)
cat("AUC: ", auc_value, "\n")

```

AUC : 0.998894 L'AUC mesure la capacité globale du modèle à discriminer entre les classes. Une AUC proche de 1 (votre AUC est 99.9%) indique que le modèle a une très bonne performance de classification, capable de distinguer presque parfaitement entre les classes True et False.

#### b- Validation croisée K-Fold :

La validation croisée K-Fold permet de tester la robustesse du modèle en le validant sur différentes sous-parties du dataset.

```{r}
# Installer et charger le package cvms si ce n'est pas déjà fait
if (!require(cvms)) install.packages("cvms")
if (!require(caret)) install.packages("caret")
library(cvms)
library(caret)

# Appliquer la validation croisée K-Fold
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(data$is_genuine, k = 10)

# Calculer les performances pour chaque fold
cv_results <- lapply(folds, function(fold) {
  train_data <- data[-fold, ]
  test_data <- data[fold, ]
  
  # Ajuster le modèle sur les données d'entraînement
  model <- glm(is_genuine ~ height_left + height_right + margin_low + margin_up + length,
                family = "binomial", data = train_data)
  
  # Prédictions sur les données de test
  test_predictions <- predict(model, newdata = test_data, type = "response")
  test_classes <- ifelse(test_predictions > 0.5, 1, 0)
  
  # Calculer la matrice de confusion et les mesures de performance
  test_confusion_matrix <- table(Predicted = test_classes, Actual = test_data$is_genuine)
  precision <- test_confusion_matrix[2, 2] / (test_confusion_matrix[2, 2] + test_confusion_matrix[1, 2])
  recall <- test_confusion_matrix[2, 2] / (test_confusion_matrix[2, 2] + test_confusion_matrix[2, 1])
  specificity <- test_confusion_matrix[1, 1] / (test_confusion_matrix[1, 1] + test_confusion_matrix[1, 2])
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  return(c(precision = precision, recall = recall, specificity = specificity, f1_score = f1_score))
})

# Moyenne des résultats de validation croisée
cv_results <- do.call(rbind, cv_results)
mean_cv_results <- colMeans(cv_results)
print(mean_cv_results)

```

Le lift est une mesure souvent utilisée pour évaluer l'amélioration de la prédiction par rapport à un modèle de base (par exemple, prédire la classe majoritaire). Les valeurs élevées du lift indiquent que votre modèle offre des améliorations significatives par rapport à des méthodes naïves.

Conclusion Ces résultats montrent que votre modèle de régression logistique est très performant avec des scores de précision, de rappel, de spécificité et de F1-Score très élevés, ainsi qu'une AUC presque parfaite. Cela suggère que le modèle est bien ajusté à vos données et est capable de faire des prédictions avec une grande précision.

Prochaines Étapes Validation Externe : Si possible, testez le modèle sur un jeu de données complètement indépendant pour vérifier sa généralisation. Analyse des Résidus : Examinez les résidus pour identifier tout modèle non détecté ou problème potentiel. Exploration de Modèles Alternatifs : Bien que le modèle de régression logistique semble excellent, vous pouvez explorer d'autres modèles pour comparer les performances. Implémentation et Déploiement : Si le modèle répond à vos attentes, vous pouvez envisager de l'implémenter dans un environnement de production. Ces étapes vous aideront à assurer que votre modèle est non seulement performant mais aussi robuste et fiable dans des conditions réelles.

------------------------------------------------------------------------

Nous allons ajuster le seuil pour réduire la probabilité de classer un faux billet parmis les vrais, c'est à dire minimiser le risque de faux positifs.

```{r}
# Prédire les probabilités pour la classe positive
predicted_prob <- predict(reg_log_model, type = "response")

```

```{r}
head(predicted_prob)
```

Création d'une fonction pour calculer la spécificité

```{r}
# Fonction pour calculer la spécificité
calculate_specificity <- function(pred_probs, actual_classes, threshold) {
  predicted_classes <- ifelse(pred_probs > threshold, 1, 0)
  confusion <- table(predicted_classes, actual_classes)
  # Confusion peut avoir des noms différents selon les résultats, ajustez si nécessaire
  VN <- confusion["0", "0"]
  FP <- confusion["1", "0"]
  specificity <- VN / (VN + FP)
  return(specificity)
}

```

On teste divers seuils

```{r}
thresholds <- seq(0.10, 0.99, by = 0.01)
specificities <- sapply(thresholds, function(t) {
  calculate_specificity(predicted_prob, data$is_genuine, t)
})
```

On recherche le meilleur seuil

```{r}
best_threshold <- thresholds[which.max(specificities)]
best_specificity <- max(specificities)

cat("Meilleur seuil:", best_threshold, "\n")
cat("Spécificité à ce seuil:", best_specificity, "\n")
```

On applique ce seuil optimal à notre modèle pour obtenir les prédictions finales

```{r}
final_predictions <- ifelse(predicted_prob > best_threshold, "True", "False")
final_confusion_matrix <- table(final_predictions, Actual = data$is_genuine)
print(final_confusion_matrix)
```

Avec ce modèle et ce seuil de sensibilité, on obtient un résultat intéressant pour le nombre de faux positif. Il n'y a qu'un seul faux billet détecté comme positif sur les 1500 billets de notre jeu de donnée.

```{r}
taux_erreur <- 1/1500 * 100
print(paste(format(taux_erreur, digits = 1, nsmall = 1), "%"))
```

```{r}
# Calculer les mesures de performance
precision_2 <- final_confusion_matrix[2, 2] / (final_confusion_matrix[2, 2] + final_confusion_matrix[2, 1])
recall_2 <- final_confusion_matrix[2, 2] / (final_confusion_matrix[2, 2] + final_confusion_matrix[1, 2])
specificity_2 <- final_confusion_matrix[1, 1] / (final_confusion_matrix[1, 1] + final_confusion_matrix[2, 1])
f1_score_2 <- 2 * (precision_2 * recall_2) / (precision_2 + recall_2)
# Calculer Accuracy
model_regression_logistic2_accuracy <- (confusion_reg_log[1, 1] + confusion_reg_log[2, 2]) / sum(confusion_reg_log)      # (TP + TN) / Total
# Afficher les résultats
cat("Précision: ", precision_2, "\n")
cat("Sensibilité (Recall): ", recall_2, "\n")
cat("Spécificité: ", specificity_2, "\n")
cat("F1-Score: ", f1_score_2, "\n")
cat("Accuracy: ", model_regression_logistic2_accuracy, "\n")
```

En regardant les performances globales, on voit que nous avons effectivement améliorer la Spécificité, c'est à dire le risque de faux positifs, passant de 0.982 à 0.998. 99.8% des faux billets sont détectés. Nous avons également augmenté la précision passant de 0.991 à 0.998, cela signifie que 99.8% des billets classés dans "True" sont réellement vrais.

Mais nous avons une baisse de sensibilité passant de 0.996 à 0.95 ce qui signifie que seulement 95% des vrais billets sont identifiés et classés comme tel, donc 5% de vrais billets sont considérés comme faux par le modèle. et une baisse du F1_score passant de 0.993 à 0.97, c'est à dire un moins bon équilibre entre détection des faux et des vrais billets dans le global.

Nous allons tester d'autres méthodes de classification pour comparer les modèles et leur performance.

Méthode RANDOM FOREST

```{r}
# Charger les packages nécessaires
if (!require(randomForest)) install.packages("randomForest")
library(randomForest)
library(caret)

# Préparer les données
features <- data[ , names(data) != "is_genuine"]
target <- as.factor(data$is_genuine)

# Diviser les données en ensembles d'entraînement et de test
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Préparer les caractéristiques et la cible pour l'entraînement et le test
trainFeatures <- trainData[ , names(trainData) != "is_genuine"]
trainTarget <- as.factor(trainData$is_genuine)
testFeatures <- testData[ , names(testData) != "is_genuine"]
testTarget <- as.factor(testData$is_genuine)

# Entraîner le modèle Random Forest pour classification
rf_model <- randomForest(x = trainFeatures, y = trainTarget, ntree = 100, importance = TRUE)
print(rf_model)

# Faire des prédictions sur l'ensemble de test
predictions <- predict(rf_model, testFeatures)

# Convertir les prédictions en facteur avec les niveaux de testTarget
predictions <- factor(predictions, levels = levels(testTarget))

# Évaluer le modèle
confusionMatrix(predictions, testTarget)

```

Utilisation des variables déterminées par StepAIC pour voir si cela améliore le modèle Randomforest

```{r}
# Préparer les données pour Random Forest en utilisant les variables sélectionnées
selected_vars <- c("height_left", "height_right", "margin_low", "margin_up", "length")

trainData_selected <- trainData[, c(selected_vars, "is_genuine")]
testData_selected <- testData[, c(selected_vars, "is_genuine")]

```

```{r}
# Convertir 'is_genuine' en facteur
trainData_selected$is_genuine <- as.factor(trainData_selected$is_genuine)
testData_selected$is_genuine <- as.factor(testData_selected$is_genuine)

```

```{r}
library(randomForest)

# Construire le modèle Random Forest
rf_model_2 <- randomForest(is_genuine ~ ., data = trainData_selected, ntree = 100, importance = TRUE)

# Afficher les résultats du modèle
print(rf_model_2)

```

```{r}
# Prédictions sur le jeu de test
predictions_2 <- predict(rf_model_2, newdata = testData_selected)

# Calculer la matrice de confusion et d'autres statistiques
library(caret)
confusionMatrix(predictions_2, testData_selected$is_genuine)

```

Les statistiques sont les mêmes qu'avec l'intégralité des variables.

Essayons en modifiant les hyperparamètres de la méthode RandoForest

```{r}
# Charger les packages nécessaires
if (!require(randomForest)) install.packages("randomForest")
if (!require(caret)) install.packages("caret")
library(randomForest)
library(caret)

# Préparer les données
features <- data[ , names(data) != "is_genuine"]
target <- as.factor(data$is_genuine)

# Diviser les données en ensembles d'entraînement et de test
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Préparer les caractéristiques et la cible pour l'entraînement et le test
trainFeatures <- trainData[ , names(trainData) != "is_genuine"]
trainTarget <- as.factor(trainData$is_genuine)
testFeatures <- testData[ , names(testData) != "is_genuine"]
testTarget <- as.factor(testData$is_genuine)

# Définir la grille de recherche des hyperparamètres
tuneGrid <- expand.grid(
  mtry = c(2, 3, 4)  # Nombre de variables à essayer à chaque split
)

# Définir le contrôle de la validation croisée
control <- trainControl(method = "cv", number = 5)

# Entraîner le modèle avec recherche des hyperparamètres
rf_model_tuned <- train(
  x = trainFeatures,
  y = trainTarget,
  method = "rf",
  trControl = control,
  tuneGrid = tuneGrid,
  importance = TRUE
)

# Afficher les meilleurs paramètres trouvés
print(rf_model_tuned$bestTune)

# Faire des prédictions sur l'ensemble de test avec le meilleur modèle
predictions <- predict(rf_model_tuned, testFeatures)

# Évaluer le modèle
cm_random_forest <- confusionMatrix(predictions, testTarget)
cm_random_forest

```

```{r}
model_random_forest_accuracy <- cm_random_forest$overall["Accuracy"]
model_random_forest_accuracy
```

Essayons avec la méthode de Réseau Neuronal

```{r}
print(head(data))
print(summary(data))
print(str(data))
```

------------------------------------------------------------------------

```{r}
library(reticulate)

# Utilisez le nouvel environnement Conda
use_condaenv("tf-env", required = TRUE)

# Vérifiez la configuration de Python
py_config()

```

```{r}
library(keras)
use_condaenv("tf-env", required=T)
```

Check if graphics card (GPU) is detected:

```{r}
library(tensorflow)
tf$python$client$device_lib$list_local_devices()
```

```{r}
# Normalisation des variables continues
df_normalized <- data %>%
  mutate(across(c(diagonal, height_left, height_right, margin_low, margin_up, length), scale))

# Diviser les données en ensembles d'entraînement et de test
set.seed(52)  # Pour la reproductibilité
train_indices <- sample(1:nrow(df_normalized), 0.8 * nrow(df_normalized))
train_data <- df_normalized[train_indices, ]
test_data <- df_normalized[-train_indices, ]

# Préparer les entrées et sorties
x_train <- as.matrix(train_data %>% dplyr::select(everything(), -is_genuine))
y_train <- as.matrix(train_data$is_genuine)

x_test <- as.matrix(test_data %>% dplyr::select(everything(), -is_genuine))
y_test <- as.matrix(test_data$is_genuine)

```

```{r}
library(keras)

# Créer le modèle
model_neuronal_1 <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Compiler le modèle
model_neuronal_1 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

```

```{r}
# Évaluer le modèle
score1 <- model_neuronal_1 %>% evaluate(x_test, y_test)
print(score1)
```

```{r}
# Créer le modèle
model_neuronal_2 <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Compiler le modèle
model_neuronal_2 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

```

```{r}
# Évaluer le modèle
score2 <- model_neuronal_2 %>% evaluate(x_test, y_test)
print(score2)
```

```{r}
# Créer un modèle plus simple
model_neuronal_simplified <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_simplified %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Entraîner le modèle
history <- model_neuronal_simplified %>% fit(
  x_train, y_train,
  epochs = 20,  # Augmenter le nombre d'époques si nécessaire
  batch_size = 32,
  validation_split = 0.2
)

# Évaluer le modèle
score_simplified <- model_neuronal_simplified %>% evaluate(x_test, y_test)
print(score_simplified)

```

```{r}
# Créer un modèle avec Dropout
model_neuronal_dropout <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_dropout %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Entraîner le modèle
history_dropout <- model_neuronal_dropout %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2
)

# Évaluer le modèle
score_dropout <- model_neuronal_dropout %>% evaluate(x_test, y_test)
print(score_dropout)

```

```{r}
library(keras)

# Créer un modèle avec Dropout
model_neuronal_3 <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_3 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Ajouter un callback pour l'arrêt précoce
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 3,
  restore_best_weights = TRUE
)

# Entraîner le modèle
history <- model_neuronal_3 %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(early_stopping)
)

# Évaluer le modèle
score3 <- model_neuronal_3 %>% evaluate(x_test, y_test)
print(score3)

```

```{r}
library(keras)

# Créer un modèle avec Dropout
model_neuronal_4 <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_4 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Ajouter un callback pour l'arrêt précoce
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 3,
  restore_best_weights = TRUE
)

# Entraîner le modèle
history <- model_neuronal_4 %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(early_stopping)
)

# Évaluer le modèle
score4 <- model_neuronal_4 %>% evaluate(x_test, y_test)
print(score4)

```

```{r}
# 1. Faire des prédictions sur les données de test
y_pred <- model_neuronal_4 %>% predict(x_test)

# 2. Convertir les probabilités en classes (0 ou 1)
y_pred_class <- ifelse(y_pred > 0.5, 1, 0)

# 3. Créer la matrice de confusion
conf_matrix <- table(Predicted = y_pred_class, Actual = y_test)

# 4. Afficher la matrice de confusion
print(conf_matrix)

```

```{r}
# Installer et charger le package caret si ce n'est pas déjà fait
# install.packages("caret")
library(caret)

# Utiliser caret pour obtenir une matrice de confusion plus détaillée
cm_reseau_neuronal <- confusionMatrix(as.factor(y_pred_class), as.factor(y_test))
cm_reseau_neuronal
```

COMPARAISON DES PERFORMANCES ENTRE LES MODELES

```{r}
model_neuronal_accuracy <- cm_reseau_neuronal$overall["Accuracy"]
model_neuronal_accuracy

```

Sauvegarde du modèle à utiliser

```{r}
# Sauvegarder le modèle neuronales (librairie keras)
model_neuronal_4 %>% save_model_hdf5("model_faux_billets.h5")

```

```{r}
# Sauvegarder le modèle et le seuil
saveRDS(reg_log_model, "modele_regression_logistique.rds")
saveRDS(best_threshold, "seuil_optimal.rds")
```
