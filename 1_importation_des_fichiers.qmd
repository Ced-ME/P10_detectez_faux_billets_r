---
title: "Détecter des faux billets"
author: "CME"
format: html
editor: visual
---

## I- Contexte

Vous êtes consultant Data Analyst dans une entreprise spécialisée dans la data. Votre entreprise a décroché une prestation en régie au sein de l’**Organisation nationale de lutte contre le faux-monnayage (ONCFM)**.

![](img/logo_oncfm.png)

Cette institution a pour objectif de mettre en place des méthodes d’identification des contrefaçons des billets en euros. Ils font donc appel à vous, spécialiste de la data, pour mettre en place une modélisation qui serait capable d’identifier automatiquement les vrais des faux billets. Et ce à partir simplement de certaines dimensions du billet et des éléments qui le composent.

Voici le [cahier des charges de l’ONCFM](doc/cahier_des_charges.pdf) ainsi que le [jeu de données](data_raw/billets.csv)

Le client souhaite que vous travailliez directement depuis ses locaux sous la responsabilité de Marie, responsable du projet d’analyse de données à l’ONCFM. Elle vous laissera une grande autonomie pendant votre mission, et vous demande simplement que vous lui présentiez vos résultats une fois la mission terminée. Elle souhaite voir quels sont les traitements et analyses que vous avez réalisés en amont, les différentes pistes explorées pour la construction de l’algorithme, ainsi que le modèle final retenu.

Après avoir lu en détail le cahier des charges, vous vous préparez à vous rendre à l’ONCFM pour prendre vos nouvelles fonctions. Vous notez tout de même un post-it qui se trouve sur le coin de votre bureau, laissé par un de vos collègues :

## II- Importation des fichiers

```{r}
data <- read.csv("data_raw/billets.csv", sep =";")
```

## III- Résumé des datas

```{r}
summary(data)
```

Nous avons un dataframe de 7 colonnes et 1 500 lignes 1 colonne de type character 6 colonnes numériques

## IV- Description des variables

```{r}
if (!require(skimr)) install.packages("skimr")
library(skimr)

skim(data)

```

Nous avons 37 valeurs manquantes dans la colonne margin_low

```{r}
# Afficher les lignes où 'margin_low' est NA
missing_margin_low <- data[is.na(data$margin_low), ]
print(missing_margin_low)

```

```{r}
valeur_unique <- unique(data$is_genuine)
print(valeur_unique)
```

Nous avons 2 valeurs uniques dans la colonne is_genuine =\> True ou False

```{r}
valeur_compte <- table(data$is_genuine)
print(valeur_compte)
```

Il y a **500 valeurs False** et **1 000 valeurs True**

```{r}
# Calcul des pourcentages
pourcentage <- round(valeur_compte / sum(valeur_compte) * 100, 1)
pourcentage_labels <- paste(pourcentage, "%")

# Définir les marges du graphique (gauche, droite, bas, haut)
par(mar = c(1, 1, 1, 1))

# Définir le rapport d'aspect pour le graphique circulaire
par(pty = "s")

# Création du pie chart avec les labels (sans les valeurs)
pie(valeur_compte, 
    labels = names(valeur_compte), 
    main = "Distribution des valeurs de is_genuine", 
    col = c("purple", "skyblue"), 
    border = "white")

# Calcul des positions des étiquettes
positions <- cumsum(valeur_compte) - valeur_compte / 2
text(x = cos(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     y = sin(2 * pi * positions / sum(valeur_compte)) * 0.5, 
     labels = pourcentage_labels, 
     cex = 1.2, col = "white")

```

### En résumé

Nous avons un tableau regroupant les données de 1 500 billets\

1 colonne décrivant s'il s'agit de vrais ou faux billets :\
- il y a 1 000 vrais billets 66.7% et 500 faux billets 33.3%\

6 colonne décrivants le format de ces billets :\
- diagonale\
- hauteur gauche\
- hauteur droite\
- marge basse\
- marge haute\
- longueur\

***37 billets*** n'ont pas l'information de la marge basse dans le tableau.

Nous allons aggréger les données sur la colonnne is_genuine\
Afficher la valeur moyenne de chaque variable pour les lignes False et True afin de voir si nous pouvons observer des différences significatives sur une ou plusieurs variables.

```{r}
if (!require(dplyr)) install.packages("dplyr")

library(dplyr)
```

```{r}
resultats <- data %>%
  group_by(is_genuine) %>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE)))

print(resultats)
```

Il y a bien quelques différences de mesures entre les vrais et faux billets mais il est difficile d'utiliser ces moyennes pour notre objectif final qui sera de déterminer d'après les mesures de billets s'il s'agit de "vrais" ou de "faux".

## V- Remlacement des NaN de la colonne "margin_low"

Pour commencer il est nécessaire de traiter les valeurs manquantes de notre jeu de données.

Nous avons la possibilité de retirer ces 37 lignes ou bien de déterminer les valeurs manquantes de la colonne "margin_low" en fonction des autres variables présentes à l'aide d'une régréssion linéaire.

Regréssion linéaire multiple : Nous utilisons toutes les variables pour commencer et nous allons voir si elles sont toutes significatives pour la détermination de "margin_low". Nous fixons notre seuil de test à 5% Toutes les p-valeur inférieure à 0.05 seront donc considérée comme significatives. Si toutes les variables ne sont pas significatives, nous utiliserons la méthode backward et retirerons une à une toutes les variables non significatives.

```{r}
# Supprimer les lignes où 'margin_low' est NaN
data_without_na <- data[!is.na(data$margin_low), ]

```

```{r}
reg_multi <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data_without_na)
summary(reg_multi)
```

Les variables sont toutes significatives car leur p-value est inférieure à 5% (0.05) le niveau de test que nous souhaitons. Nous conservons donc toutes les variables pour la détermination de 'margin_low'

Nous pouvons tester le modèle sous forme de prédiction en renseignant toutes les variables pour déterminer une valeur à margin_low

```{r}
prev <- data.frame(diagonal=169, height_left=105, height_right=105, margin_up=3, length=115)
marg_low_prev <- predict(reg_multi, prev)
round(marg_low_prev,digits=2)
```

```{r}
#niveau de test
alpha <- 0.05
#nombre d'individus
n <- dim(data_without_na)[1]
#nombre de variables
p <- 6
```

```{r}
#analyse sur les valeurs atypique ou influantes
analyses <- data.frame(obs=1:n)
```

```{r}
#calcul des leviers
analyses$levier <- hat(model.matrix(reg_multi))
seuil_levier <- 2*p/n
```

```{r}
library(ggplot2)
ggplot(data=analyses, aes(x=obs,y=levier))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_levier, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Leviers")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

```{r}
#pour récupérer les numéros des observations pour lesquels les leviers sont supérieurs au seuil
idl <- analyses$levier>seuil_levier
head(idl)
analyses$levier[idl]
```

Calculer les résidus studentisés

```{r}
analyses$rstudent <- rstudent(reg_multi)
seuil_rstudent <- qt(1-alpha/2,n-p-1)
```

```{r}
ggplot(data=analyses, aes(x=obs,y=rstudent))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=-seuil_rstudent, col="red")+
  geom_hline(yintercept=seuil_rstudent, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Résidus studentisés")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

Pour récupérer la distance de Cook

```{r}
influence<-influence.measures(reg_multi)
names(influence)
colnames(influence$infmat)
```

```{r}
analyses$dcook <- influence$infmat[,"cook.d"]
seuil_dcook <- 4/(n-p)
```

```{r}
seuil_dcook
```

```{r}
ggplot(data=analyses, aes(x=obs,y=dcook))+
  geom_bar(stat='identity', fill="steelblue")+
  geom_hline(yintercept=seuil_dcook, col="red")+
  theme_minimal()+
  xlab("Observation")+
  ylab("Distance de Cook")+
  scale_x_continuous(breaks=seq(0,n,by=5))
```

Recherche de colinéarité (commande VIF)

```{r}
# Installation du package
install.packages("car")

# Charger le package
library(car)

vif(reg_multi)
```

Tous les VIF étant inférieur à 10, il ne semble pas y avoir de problème de colinéarité

On peut également tester l'homoscédasticité, c'est à dire la constance de la variance des erreurs On peut donc réaliser le test de Breusch-Pagan

```{R}
install.packages("lmtest")
library(lmtest)

bptest(reg_multi)
```

La p-valeur est inférieur à 0.05 et proche de 0 On rejete donc l'hypothèse nulle H0.\
L'hypothèse nulle de ce test est que les erreurs ont une variance constante, c'est-à-dire que l'hétéroscédasticité n'est pas présente dans les résidus du modèle.\
En d'autres termes, l'hypothèse nulle affirme que les résidus sont homoscédastiques.

Hypothèse nulle (H0) : Les résidus du modèle de régression sont homoscédastiques (la variance des erreurs est constante).\
Hypothèse alternative (H1) : Les résidus du modèle de régression ne sont pas homoscédastiques (il y a de l'hétéroscédasticité).

La p-value obtenue est extrêmement petite (7.76e-16), bien en dessous du seuil de 0,05, qui est généralement utilisé pour les tests statistiques.

il y a des preuves significatives d'hétéroscédasticité dans le modèle de régression.\
En d'autres termes, la variance des erreurs n'est pas constante et cela peut biaiser les estimations de l'erreur standard des coefficients de régression, ce qui peut à son tour affecter la validité des tests d'hypothèses et des intervalles de confiance.

Autre point Pour tester la normalité des résidus, nous pouvons utiliser le test de shapiro

```{r}
shapiro.test(reg_multi$residuals)
```

L'hypothèse de normalité est remise en cause car la p-value est inférieure à 0.05 et proche de 0

Nous aurions pu vouloir sélectionner automatiquement un modèle avec l'ensemble des variables avec le package stat

```{r}
reg_null <- lm(margin_low~1, data=data_without_na)
reg_tot <- lm(margin_low~diagonal+height_left+height_right+margin_up+length, data=data)
```

```{r}
#Méthode backward
reg_backward <- step(reg_tot, direction="backward")
```

Le résultat nous montre que toutes les variables explicatives sont importantes dans le modèle pour prédire margin_low. la suppression de length à l'impact le plus négatif sur le modèle augmentant considérablement le RSS (somme des carrés des résidus) chaque suppression rend le modèle moins performant. Le modèle final sera le modèle complet

ANCIENNE PARTIE A REVOIR

## V- Remplacement des données manquantes

```{r}
# Prédire les valeurs manquantes
predicted_values <- predict(reg_multi, newdata = missing_margin_low)
```

```{r}
predicted_values
```

```{r}
# Étape 3: Remplacer les NA par les valeurs prédites
data$margin_low[is.na(data$margin_low)] <- predicted_values
```

```{r}
# Voir le résultat
summary(data$margin_low)
```

```{r}
missing_margin_low
```

On contrôle ici que les valeurs déterminées par le modèle ont bien remplacées les NA de notre jeu de donnée "data"

```{r}
indices <- rownames(missing_margin_low)
lignes_data <- data[indices, ]
print(lignes_data)

```

```{r}
skim(data)
```

```{r}
# Obtenir les résidus du modèle
residus <- residuals(reg_multi)

# Résumé des résidus
summary(residus)

# Visualiser les résidus
hist(residus, main="Distribution des résidus", xlab="Résidus")

```

------------------------------------------------------------------------

## VI- ACP et Clustering

### 1- Recherche de valeurs aberrantes

```{r}
# Boxplot pour chaque variable numérique
data_numeric <- data %>% select(where(is.numeric))

# Afficher boxplots
par(mfrow = c(2, 3))  # Adapter le nombre de graphiques selon le nombre de variables
for (col in colnames(data_numeric)) {
  boxplot(data_numeric[[col]], main = col, ylab = col)
}

```

```{r}
# Scatter plot entre deux variables
plot(data$diagonal, data$height_left, main = "Diagonale vs. Hauteur gauche", xlab = "Diagonale", ylab = "Hauteur gauche")

```

#### a- Méthode IQR

```{r}
# Calculer l'IQR pour chaque colonne
iqr <- function(x) {
  q3 <- quantile(x, 0.75)
  q1 <- quantile(x, 0.25)
  q3 - q1
}

# Détecter les outliers pour chaque colonne
outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr_value <- iqr(x)
  lower_bound <- q1 - 1.5 * iqr_value
  upper_bound <- q3 + 1.5 * iqr_value
  x < lower_bound | x > upper_bound
}
```

```{r}
# Appliquer la fonction pour détecter les outliers
iqr_outliers_data <- data_numeric %>%
  mutate(across(everything(), ~ outliers(.)))

# Afficher les outliers
head(iqr_outliers_data)
```

```{r}
# Afficher les indices des outliers
iqr_outliers_indices <- which(rowSums(iqr_outliers_data) > 0)
print(iqr_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers
data_with_iqr_outliers <- data[iqr_outliers_indices, ]

print(data_with_iqr_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
iqr_count_by_is_genuine <- data_with_iqr_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(iqr_count_by_is_genuine)

```

Il y a 53 lignes d'outliers avec la méthode IQR\
40 lignes concernent des faux billets\
13 lignes concernent de vrais billets

#### b- Méthode Z_score

```{r}
if (!require(tidyverse)) install.packages("tidyverse")

# Charger les bibliothèques nécessaires
library(tidyverse)

# Sélectionner les colonnes numériques
data_numeric <- data %>% select(where(is.numeric))

# Calculer les scores Z
data_z <- scale(data_numeric)

# Vérifier les scores Z
head(data_z)

```

```{r}
summary(data_z)
```

```{r}
# Ajouter les scores Z au dataframe original en conservant la colonne de caractères
data_z_full <- cbind(Row_Index = 1:nrow(data), data %>% select(where(is.character)), as.data.frame(data_z))

# Afficher les premières lignes du nouveau dataframe
head(data_z_full)

```

```{r}
# Visualiser la distribution des scores Z pour la colonne 'diagonal'
ggplot(as.data.frame(data_z), aes(x=diagonal)) + 
  geom_histogram(binwidth=0.5, fill="blue", color="black") + 
  theme_minimal() +
  labs(title="Distribution des scores Z pour la colonne 'diagonal'")
```

On fixe à +3 et -3 la limite de z_score pour identifier les outliers

Puis on affiche les lignes pour lesquels il y a au moins une variables considérée comme outlier

```{r}
# Identifier les lignes avec des outliers (Z > 3 ou Z < -3)
z_outliers <- data_z_full %>%
  filter(apply(data_z, 1, function(row) any(row > 3 | row < -3)))

# Afficher les outliers
print(z_outliers)

```

```{r}
# Afficher les indices des outliers
z_outliers_indices <- z_outliers$Row_Index
print(z_outliers_indices)
```

```{r}
# Afficher les lignes du dataframe original contenant des outliers avec le méthode du z-score
data_with_z_outliers <- data[z_outliers_indices, ]

print(data_with_z_outliers)
```

```{r}
library(dplyr)

# Comptage du nombre de lignes par groupe dans la colonne is_genuine
z_count_by_is_genuine <- z_outliers %>%
  group_by(is_genuine) %>%
  summarise(count = n())

# Affichage du résultat
print(z_count_by_is_genuine)

```

Il y a 24 lignes d'outliers avec la méthode du Z_score\
17 lignes concernent des faux billets\
7 lignes concernent de vrais billets

### 2- Suppression des lignes d'outliers

Je vais commencer mon analyse en supprimant les outliers identifiés par la méthode du Z_Score\
Je vais donc supprimer 24 lignes du dataframe original "data"\
je le nommerai "data_without_ouliers_z"

```{r}
data_without_ouliers_z <- data[-z_outliers_indices, ]
data_without_ouliers_z
```

### 3- ACP

```{r}
if (!require(FactoMineR)) install.packages("FactoMineR")
if (!require(factoextra)) install.packages("factoextra")

library(FactoMineR)
library(factoextra)
```

```{r}
# Extraire les colonnes numériques
datanum_without_ouliers_z <- Filter(is.numeric, data_without_ouliers_z)
```

```{r}
# Réaliser l'ACP sans spécifier ncp
resultat_acp <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, graph = FALSE)

# Extraire les valeurs propres
valeurs_propres <- resultat_acp$eig[,1]
```

```{r}
# Calculer les pourcentages d'inertie expliquée par chaque composante
pourcentage_inertie <- 100 * valeurs_propres / sum(valeurs_propres)
# Calculer l'inertie cumulée
cumulative_inertia <- cumsum(pourcentage_inertie)
```

```{r}
# Créer un graphique avec un seul appel
par(mfrow = c(1, 1)) # Assurez-vous d'avoir une seule fenêtre graphique

# Créer un plot avec des barres pour les pourcentages d'inertie
barplot(pourcentage_inertie, main = "Éboulis des Composantes Principales",
        xlab = "Composante Principale", ylab = "Pourcentage d'Inertie",
        col = "lightblue", border = "blue", ylim = c(0, 100))

# Ajouter la courbe cumulée en superposition sur le même graphique
# Recréer le graphique avec la courbe cumulée en utilisant `plot` pour ne pas effacer les barres
par(new = TRUE) # Permet de superposer le nouveau graphique sur le précédent
plot(cumulative_inertia, type = "o", col = "red", pch = 19, 
     ylim = c(0, 100), xlab = "", ylab = "", axes = FALSE)

# Ajouter une légende
legend("topright", legend = c("Inertie Cumulée (%)"), col = "red", pch = 19, inset = c(0, 0.5))
```

```{r}
# on choisit 4 composantes principales qui représentent 80% de la variance
resultat_acp_optimal <- PCA(datanum_without_ouliers_z, scale.unit = TRUE, ncp = 4, graph = FALSE)

```

```{r}
# Visualiser les individus et les variables pour les premières et dernières combinaisons de composantes
pairs_to_plot <- list(c(1, 2), c(3, 4))  # Choisir les paires de composantes à afficher

# Visualiser les individus et les variables pour les différentes combinaisons de composantes
for (pair in pairs_to_plot) {
# Visualiser les individus
  print(fviz_pca_ind(resultat_acp_optimal, 
                     axes = pair, 
                     col.ind = "cos2", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                     repel = TRUE) +
        ggtitle(paste("Visualisation des Individus - Composantes", pair[1], "et", pair[2])))
  
  # Visualiser les variables
  print(fviz_pca_var(resultat_acp_optimal, 
                     axes = pair, 
                     col.var = "contrib", 
                     gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")) +
        ggtitle(paste("Visualisation des Variables - Composantes", pair[1], "et", pair[2])))
}
```

Axes principaux :\
Dim1 (43.3%) : Cet axe explique 43.3% de la variance totale des données.\
Dim2 (17%) : Cet axe explique 17% de la variance totale des données.

Dim1 On observe :\
- une forte contribution positibe de la marge basse (margin_low)\
- une forte contribution négative de la longueur (length)\
une forte valeur de Dim1 indiquera un longueur de billet faible avec une marge basse élevée

Dim2 On observe :\
- une forte contribution positibe de la diagonale Une forte valeur de Dim2 indiquera une valeur de diagonale élevée

```{r}
# Extraire les scores des individus
scores <- resultat_acp_optimal$ind$coord

```

```{r}
head(scores)
```

### 4- Clustering

```{r}
# Définir une plage de nombres de clusters à tester
k_values <- 1:10  # Tester de 1 à 10 clusters
wss <- numeric(length(k_values))  # Pour stocker la somme des carrés intra-cluster

# Calculer l'inertie pour chaque nombre de clusters
for (k in k_values) {
  kmeans_result <- kmeans(scores, centers = k, nstart = 25)  # nstart pour la reproductibilité
  wss[k] <- kmeans_result$tot.withinss
}

# Tracer la méthode du coude
plot(k_values, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Nombre de Clusters", ylab = "Somme des Carrés Intra-Cluster",
     main = "Méthode du Coude pour K-means Clustering")
```

TEST AVEC 2 CLUSTERS

```{r}
# Appliquer K-means clustering avec 2 clusters
kmeans_result <- kmeans(scores, centers = 2, nstart = 23)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters <- fviz_cluster(kmeans_result, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters <- plot_clusters + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 2 Clusters")

# Ajouter les centres des clusters au graphique
centroids <- kmeans_result$centers

plot_clusters <- plot_clusters + 
  geom_point(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids), aes(x = Dim.1, y = Dim.2, label = rownames(centroids)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters)

```

```{r}
# Centres des clusters
print(kmeans_result$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters <- data.frame(data_without_ouliers_z, Cluster = kmeans_result$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters)

```

```{r}
count_values  <- data_with_clusters %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

TEST AVEC 3 CLUSTERS

```{r}
# Appliquer K-means clustering avec 3 clusters
kmeans_result_2 <- kmeans(scores, centers = 3, nstart = 33)  # nstart pour la reproductibilité

```

```{r}
# Installer et charger le package factoextra si ce n'est pas déjà fait
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)

# Installer et charger le package ggplot2 si ce n'est pas déjà fait
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Visualiser les clusters dans les deux premières composantes principales
plot_clusters_2 <- fviz_cluster(kmeans_result_2, data = scores,
                              ellipse.type = "euclid",  # Ajouter des ellipses autour des clusters
                              ggtheme = theme_minimal())

# Modifier les étiquettes des axes pour refléter la variance expliquée
plot_clusters_2 <- plot_clusters_2 + 
  labs(x = "Dim.1 (43.3% de variance)", y = "Dim.2 (17% de variance)") +
  ggtitle("Clustering K-means avec 3 Clusters")

# Ajouter les centres des clusters au graphique
centroids_2 <- kmeans_result_2$centers

plot_clusters_2 <- plot_clusters_2 + 
  geom_point(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2), 
             color = "red", size = 5, shape = 8) +  # Points rouges pour les centres
  geom_text(data = as.data.frame(centroids_2), aes(x = Dim.1, y = Dim.2, label = rownames(centroids_2)),
            color = "black", vjust = -1, hjust = 1)  # Étiquettes pour les centres

# Afficher le graphique final
print(plot_clusters_2)

```

```{r}
# Centres des clusters
print(kmeans_result_2$centers)

```

```{r}
# Taille de chaque cluster
print(kmeans_result_2$size)

```

```{r}
# Ajouter les résultats du clustering à un DataFrame
data_with_clusters_2 <- data.frame(data_without_ouliers_z, Cluster = kmeans_result_2$cluster)

# Afficher les premières lignes du DataFrame avec les clusters
head(data_with_clusters_2)

```

```{r}
count_values  <- data_with_clusters_2 %>%
  group_by(Cluster, is_genuine) %>%
  count()

print(count_values)
  
```

Le but était de vérifier comment se comportait le modèle avec 3 clusters. L'idéal aurait été un clustering avec 100 % de vrai dans un cluster 100 % de faux dans un autre cluster et les douteux dans un 3ème cluster

## VII- Régression Logistique

### 1- Remplacement des valeurs de "is_genuine" par "0" et "1"

Nous allons commencer par remplacer "False" par "0" et "True" par "1" afin que "is_genuine" contienne des valeurs numériques binaires.

```{r}
# Convertir 'is_genuine' en binaire : True -> 1 et False -> 0
data$is_genuine <- ifelse(data$is_genuine == "True", 1, 0)

# Vérifiez les premiers enregistrements pour confirmer la transformation
head(data$is_genuine)

```

### 2- Régréssion logistique complète (toutes les variables)

```{r}
reg_log <- glm(is_genuine~diagonal+height_left+height_right+margin_low+margin_up+length,
family="binomial",data=data)
summary(reg_log)
```

Certaines des variables obtenues ont des p-valeurs qui sont inférieures au niveau de test de 5 %, ce qui nous indique qu'elles sont bien significatives. Certaines autres ne sont pas en dessous de ce seuil.

On peut donc passer sur une procédure de sélection en retirant les variables non significatives au fur et à mesure, mais nous pouvons aussi sélectionner automatiquement un modèle avec une commande telle que stepAIC , qui sélectionne de manière automatique un modèle en se basant sur le critère AIC.

### 3- Sélection du modèle (méthode AIC)

```{r}
library(MASS)
stepAIC(reg_log, direction = "both")
```

La procédure de sélection de modèle nous a permis de déterminer que les variables "diagonal" et "height_left" n'étaient pas nécessaires pour le modèle final. Les variables restantes ("height_right", "margin_low", "margin_up", et "length") sont significatives et ont un impact important sur le modèle.\
Nous avons réussi à réduire l'AIC, indiquant que le modèle final est plus parcimonieux tout en conservant une bonne qualité d'ajustement.

Analyse des coefficients : 3 variables ont une valeur absolue élevée ce qui indique qu'elles ont plus d'importance et d'influence dans la prédiction. - "margin_up" a un coeff négatif de -10.167 ce qui indique que plus la marge haute haute augmente, plus il y a de chance qu'il s'agisse d'un faux billet. - "margin_low" avec un coeff négatif également de -5.794 a également une influence importante et indique que l'augmentation de la marge basse augmente les probabilités qu'il s'agisse d'un faux billet. - "length" a un coeff positif de 5.920 qui indique à l'inverse que l'augmentation de la longueur du billet augmente la probabilité qu'il s'agisse d'un vrai billet.

Calculons les Odds Ratios pour indiquer comment les changements dans les variables indépendantes affectent la probabilité de vrais ou faux billets :

```{r}
final_model <- glm(is_genuine ~ height_left + height_right + margin_low + 
    margin_up + length,
family="binomial",data=data)
summary(final_model)
```

```{r}
# Récupérer les coefficients du modèle final
coefficients <- coef(final_model)

# Calculer les odds ratios en exponentiant les coefficients
odds_ratios <- exp(coefficients)

# Afficher les odds ratios sans exposant
formatted_odds_ratios <- formatC(odds_ratios, format = "f", digits = 5)


# Imprimer les odds ratios formatés
formatted_odds_ratios

```

height_left: 0.17986 Une augmentation d'une unité de height_left diminue les chances que le billet soit authentique d'environ 82% (puisque 0.17986\<1)

height_right: 0.10402 Une augmentation d'une unité de height_right diminue les chances que le billet soit authentique d'environ 90%.

margin_low: 0.00305 Une augmentation d'une unité de margin_low diminue les chances que le billet soit authentique d'environ 99.7%.

margin_up: 0.00004 Une augmentation d'une unité de margin_up diminue fortement les chances que le billet soit authentique.

length: 372.5465 Une augmentation d'une unité de length augmente de manière significative (par 372 fois) les chances que le billet soit authentique.

### 4- Evaluation des performances du modèle

#### a- Métrique de performance

##### Matrice de confusion

La matrice de confusion vous permet de comparer les prédictions du modèle avec les valeurs réelles.

```{r}
# Prédictions du modèle
predictions <- predict(final_model, type = "response")

# Convertir les probabilités en classes (0 ou 1) en utilisant un seuil de 0.5
predicted_classes <- ifelse(predictions > 0.5, "True", "False")

# Créer la matrice de confusion
confusion_matrix <- table(Predicted = predicted_classes, Actual = data$is_genuine)
print(confusion_matrix)

```

Vrais Négatifs (VN) : 491\
Faux Négatifs (FN) : 4\
Faux Positifs (FP) : 9\
Vrais Positifs (VP) : 996

La matrice de confusion montre que votre modèle a une très bonne performance en classant les cas True et False correctement avec peu d'erreurs.

##### Mesures de performance des positifs

Précision : Proportion des prédictions correctes parmi toutes les prédictions. VP/(VP+FP)\
Sensibilité (Recall) : Proportion des vrais positifs parmi tous les cas positifs réels. VP/(VP+FN)\
Spécificité : Proportion des vrais négatifs parmi tous les cas négatifs réels. VN/(VN+FP)\
F1-Score : Moyenne harmonique de la précision et de la sensibilité. 2x((precision x recall)/(precision + recall))

```{r}
# Calculer les mesures de performance
precision <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[2, 1])
recall <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[1, 2])
specificity <- confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[2, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)

# Afficher les résultats
cat("Précision: ", precision, "\n")
cat("Sensibilité (Recall): ", recall, "\n")
cat("Spécificité: ", specificity, "\n")
cat("F1-Score: ", f1_score, "\n")

```

Précision (Precision) : 0.991 Cela signifie que parmi toutes les instances que le modèle a classées comme True, 99.1% sont effectivement True. C'est un excellent résultat, indiquant que le modèle est très précis dans ses prédictions positives.

Sensibilité (Recall) : 0.996 La sensibilité mesure la proportion des véritables True correctement identifiés par le modèle. Avec une sensibilité de 99.6%, votre modèle détecte presque tous les cas positifs.

Spécificité : 0.982 La spécificité mesure la proportion des véritables False correctement identifiés par le modèle. Avec une spécificité de 98.2%, le modèle est aussi très efficace pour identifier les cas négatifs.

F1-Score : 0.993 L'F1-Score est la moyenne harmonique de la précision et du rappel. Un F1-Score de 99.3% indique un excellent équilibre entre précision et rappel.

Mon objectif est d'obtenir un modèle avec une spécificité proche de 100% pour minimiser au maximum le risque de faux positif, il est donc important que tous les vrais négatifs soient identifier même si cela déteriore un peu la précision.

##### Courbe ROC et AUC

La courbe ROC (Receiver Operating Characteristic) est utile pour visualiser la performance du modèle à différents seuils de classification. L'AUC (Area Under the Curve) mesure la capacité globale du modèle à discriminer entre les classes.

```{r}
# Installer et charger le package pROC si ce n'est pas déjà fait
if (!require(pROC)) install.packages("pROC")
library(pROC)

# Calculer la courbe ROC
roc_curve <- roc(data$is_genuine, predictions)

# Tracer la courbe ROC
plot(roc_curve, main = "Courbe ROC")

# Calculer l'AUC
auc_value <- auc(roc_curve)
cat("AUC: ", auc_value, "\n")

```

AUC : 0.998894 L'AUC mesure la capacité globale du modèle à discriminer entre les classes. Une AUC proche de 1 (votre AUC est 99.9%) indique que le modèle a une très bonne performance de classification, capable de distinguer presque parfaitement entre les classes True et False.

#### b- Validation croisée K-Fold :

La validation croisée K-Fold permet de tester la robustesse du modèle en le validant sur différentes sous-parties du dataset.

```{r}
# Installer et charger le package cvms si ce n'est pas déjà fait
if (!require(cvms)) install.packages("cvms")
if (!require(caret)) install.packages("caret")
library(cvms)
library(caret)

# Appliquer la validation croisée K-Fold
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(data$is_genuine, k = 10)

# Calculer les performances pour chaque fold
cv_results <- lapply(folds, function(fold) {
  train_data <- data[-fold, ]
  test_data <- data[fold, ]
  
  # Ajuster le modèle sur les données d'entraînement
  model <- glm(is_genuine ~ height_left + height_right + margin_low + margin_up + length,
                family = "binomial", data = train_data)
  
  # Prédictions sur les données de test
  test_predictions <- predict(model, newdata = test_data, type = "response")
  test_classes <- ifelse(test_predictions > 0.5, 1, 0)
  
  # Calculer la matrice de confusion et les mesures de performance
  test_confusion_matrix <- table(Predicted = test_classes, Actual = test_data$is_genuine)
  precision <- test_confusion_matrix[2, 2] / (test_confusion_matrix[2, 2] + test_confusion_matrix[1, 2])
  recall <- test_confusion_matrix[2, 2] / (test_confusion_matrix[2, 2] + test_confusion_matrix[2, 1])
  specificity <- test_confusion_matrix[1, 1] / (test_confusion_matrix[1, 1] + test_confusion_matrix[1, 2])
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  return(c(precision = precision, recall = recall, specificity = specificity, f1_score = f1_score))
})

# Moyenne des résultats de validation croisée
cv_results <- do.call(rbind, cv_results)
mean_cv_results <- colMeans(cv_results)
print(mean_cv_results)

```

Le lift est une mesure souvent utilisée pour évaluer l'amélioration de la prédiction par rapport à un modèle de base (par exemple, prédire la classe majoritaire). Les valeurs élevées du lift indiquent que votre modèle offre des améliorations significatives par rapport à des méthodes naïves.

Conclusion Ces résultats montrent que votre modèle de régression logistique est très performant avec des scores de précision, de rappel, de spécificité et de F1-Score très élevés, ainsi qu'une AUC presque parfaite. Cela suggère que le modèle est bien ajusté à vos données et est capable de faire des prédictions avec une grande précision.

Prochaines Étapes Validation Externe : Si possible, testez le modèle sur un jeu de données complètement indépendant pour vérifier sa généralisation. Analyse des Résidus : Examinez les résidus pour identifier tout modèle non détecté ou problème potentiel. Exploration de Modèles Alternatifs : Bien que le modèle de régression logistique semble excellent, vous pouvez explorer d'autres modèles pour comparer les performances. Implémentation et Déploiement : Si le modèle répond à vos attentes, vous pouvez envisager de l'implémenter dans un environnement de production. Ces étapes vous aideront à assurer que votre modèle est non seulement performant mais aussi robuste et fiable dans des conditions réelles.

------------------------------------------------------------------------

Nous allons ajuster le seuil pour réduire la probabilité de classer un faux billet parmis les vrais, c'est à dire minimiser le risque de faux positifs.

```{r}
# Prédire les probabilités pour la classe positive
predicted_prob <- predict(final_model, type = "response")

```

```{r}
head(predicted_prob)
```

Création d'une fonction pour calculer la spécificité

```{r}
# Fonction pour calculer la spécificité
calculate_specificity <- function(pred_probs, actual_classes, threshold) {
  predicted_classes <- ifelse(pred_probs > threshold, 1, 0)
  confusion <- table(predicted_classes, actual_classes)
  # Confusion peut avoir des noms différents selon les résultats, ajustez si nécessaire
  VN <- confusion["0", "0"]
  FP <- confusion["1", "0"]
  specificity <- VN / (VN + FP)
  return(specificity)
}

```

On teste divers seuils

```{r}
thresholds <- seq(0.10, 0.99, by = 0.01)
specificities <- sapply(thresholds, function(t) {
  calculate_specificity(predicted_prob, data$is_genuine, t)
})
```

On recherche le meilleur seuil

```{r}
best_threshold <- thresholds[which.max(specificities)]
best_specificity <- max(specificities)

cat("Meilleur seuil:", best_threshold, "\n")
cat("Spécificité à ce seuil:", best_specificity, "\n")
```

On applique ce seuil optimal à notre modèle pour obtenir les prédictions finales

```{r}
final_predictions <- ifelse(predicted_prob > best_threshold, "True", "False")
final_confusion_matrix <- table(final_predictions, Actual = data$is_genuine)
print(final_confusion_matrix)
```

Avec ce modèle et ce seuil de sensibilité, on obtient un résultat intéressant pour le nombre de faux positif. Il n'y a qu'un seul faux billet détecté comme positif sur les 1500 billets de notre jeu de donnée.

```{r}
taux_erreur <- 1/1500 * 100
print(paste(format(taux_erreur, digits = 1, nsmall = 1), "%"))
```

```{r}
# Calculer les mesures de performance
precision_2 <- final_confusion_matrix[2, 2] / (final_confusion_matrix[2, 2] + final_confusion_matrix[2, 1])
recall_2 <- final_confusion_matrix[2, 2] / (final_confusion_matrix[2, 2] + final_confusion_matrix[1, 2])
specificity_2 <- final_confusion_matrix[1, 1] / (final_confusion_matrix[1, 1] + final_confusion_matrix[2, 1])
f1_score_2 <- 2 * (precision_2 * recall_2) / (precision_2 + recall_2)

# Afficher les résultats
cat("Précision: ", precision_2, "\n")
cat("Sensibilité (Recall): ", recall_2, "\n")
cat("Spécificité: ", specificity_2, "\n")
cat("F1-Score: ", f1_score_2, "\n")
```

En regardant les performances globales, on voit que nous avons effectivement améliorer la Spécificité, c'est à dire le risque de faux positifs, passant de 0.982 à 0.998. 99.8% des faux billets sont détectés. Nous avons également augmenté la précision passant de 0.991 à 0.998, cela signifie que 99.8% des billets classés dans "True" sont réellement vrais.

Mais nous avons une baisse de sensibilité passant de 0.996 à 0.95 ce qui signifie que seulement 95% des vrais billets sont identifiés et classés comme tel, donc 5% de vrais billets sont considérés comme faux par le modèle. et une baisse du F1_score passant de 0.993 à 0.97, c'est à dire un moins bon équilibre entre détection des faux et des vrais billets dans le global.

Nous allons tester d'autres méthodes de classification pour comparer les modèles et leur performance.

Méthode RANDOM FOREST

```{r}
# Charger les packages nécessaires
if (!require(randomForest)) install.packages("randomForest")
library(randomForest)
library(caret)

# Préparer les données
features <- data[ , names(data) != "is_genuine"]
target <- as.factor(data$is_genuine)

# Diviser les données en ensembles d'entraînement et de test
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Préparer les caractéristiques et la cible pour l'entraînement et le test
trainFeatures <- trainData[ , names(trainData) != "is_genuine"]
trainTarget <- as.factor(trainData$is_genuine)
testFeatures <- testData[ , names(testData) != "is_genuine"]
testTarget <- as.factor(testData$is_genuine)

# Entraîner le modèle Random Forest pour classification
rf_model <- randomForest(x = trainFeatures, y = trainTarget, ntree = 100, importance = TRUE)
print(rf_model)

# Faire des prédictions sur l'ensemble de test
predictions <- predict(rf_model, testFeatures)

# Convertir les prédictions en facteur avec les niveaux de testTarget
predictions <- factor(predictions, levels = levels(testTarget))

# Évaluer le modèle
confusionMatrix(predictions, testTarget)

```

Utilisation des variables déterminées par StepAIC pour voir si cela améliore le modèle Randomforest

```{r}
# Préparer les données pour Random Forest en utilisant les variables sélectionnées
selected_vars <- c("height_left", "height_right", "margin_low", "margin_up", "length")

trainData_selected <- trainData[, c(selected_vars, "is_genuine")]
testData_selected <- testData[, c(selected_vars, "is_genuine")]

```

```{r}
# Convertir 'is_genuine' en facteur
trainData_selected$is_genuine <- as.factor(trainData_selected$is_genuine)
testData_selected$is_genuine <- as.factor(testData_selected$is_genuine)

```

```{r}
library(randomForest)

# Construire le modèle Random Forest
rf_model_2 <- randomForest(is_genuine ~ ., data = trainData_selected, ntree = 100, importance = TRUE)

# Afficher les résultats du modèle
print(rf_model_2)

```

```{r}
# Prédictions sur le jeu de test
predictions_2 <- predict(rf_model_2, newdata = testData_selected)

# Calculer la matrice de confusion et d'autres statistiques
library(caret)
confusionMatrix(predictions_2, testData_selected$is_genuine)

```

Les statistiques sont les mêmes qu'avec l'intégralité des variables.

Essayons en modifiant les hyperparamètres de la méthode RandoForest

```{r}
# Charger les packages nécessaires
if (!require(randomForest)) install.packages("randomForest")
if (!require(caret)) install.packages("caret")
library(randomForest)
library(caret)

# Préparer les données
features <- data[ , names(data) != "is_genuine"]
target <- as.factor(data$is_genuine)

# Diviser les données en ensembles d'entraînement et de test
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Préparer les caractéristiques et la cible pour l'entraînement et le test
trainFeatures <- trainData[ , names(trainData) != "is_genuine"]
trainTarget <- as.factor(trainData$is_genuine)
testFeatures <- testData[ , names(testData) != "is_genuine"]
testTarget <- as.factor(testData$is_genuine)

# Définir la grille de recherche des hyperparamètres
tuneGrid <- expand.grid(
  mtry = c(2, 3, 4)  # Nombre de variables à essayer à chaque split
)

# Définir le contrôle de la validation croisée
control <- trainControl(method = "cv", number = 5)

# Entraîner le modèle avec recherche des hyperparamètres
rf_model_tuned <- train(
  x = trainFeatures,
  y = trainTarget,
  method = "rf",
  trControl = control,
  tuneGrid = tuneGrid,
  importance = TRUE
)

# Afficher les meilleurs paramètres trouvés
print(rf_model_tuned$bestTune)

# Faire des prédictions sur l'ensemble de test avec le meilleur modèle
predictions <- predict(rf_model_tuned, testFeatures)

# Évaluer le modèle
confusionMatrix(predictions, testTarget)


```

Essayons avec la méthode de Réseau Neuronal

```{r}
print(head(data))
print(summary(data))
print(str(data))
```

------------------------------------------------------------------------

```{r}
library(reticulate)

# Utilisez le nouvel environnement Conda
use_condaenv("tf-env", required = TRUE)

# Vérifiez la configuration de Python
py_config()

```

```{r}
library(keras)
use_condaenv("tf-env", required=T)
```

Check if graphics card (GPU) is detected:

```{r}
library(tensorflow)
tf$python$client$device_lib$list_local_devices()
```

```{r}
# Normalisation des variables continues
df_normalized <- data %>%
  mutate(across(c(diagonal, height_left, height_right, margin_low, margin_up, length), scale))

# Diviser les données en ensembles d'entraînement et de test
set.seed(52)  # Pour la reproductibilité
train_indices <- sample(1:nrow(df_normalized), 0.8 * nrow(df_normalized))
train_data <- df_normalized[train_indices, ]
test_data <- df_normalized[-train_indices, ]

# Préparer les entrées et sorties
x_train <- as.matrix(train_data %>% dplyr::select(everything(), -is_genuine))
y_train <- as.matrix(train_data$is_genuine)

x_test <- as.matrix(test_data %>% dplyr::select(everything(), -is_genuine))
y_test <- as.matrix(test_data$is_genuine)

```

```{r}
library(keras)

# Créer le modèle
model_neuronal <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Compiler le modèle
model_neuronal %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

```

```{r}
# Évaluer le modèle
score <- model_neuronal %>% evaluate(x_test, y_test)
print(score)
```

```{r}
# Créer le modèle
model_neuronal <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# Compiler le modèle
model_neuronal %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

```


```{r}
# Évaluer le modèle
score <- model_neuronal %>% evaluate(x_test, y_test)
print(score)
```

```{r}
# Créer un modèle plus simple
model_neuronal_simplified <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_simplified %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Entraîner le modèle
history <- model_neuronal_simplified %>% fit(
  x_train, y_train,
  epochs = 20,  # Augmenter le nombre d'époques si nécessaire
  batch_size = 32,
  validation_split = 0.2
)

# Évaluer le modèle
score_simplified <- model_neuronal_simplified %>% evaluate(x_test, y_test)
print(score_simplified)

```

```{r}
# Créer un modèle avec Dropout
model_neuronal_dropout <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model_neuronal_dropout %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Entraîner le modèle
history_dropout <- model_neuronal_dropout %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2
)

# Évaluer le modèle
score_dropout <- model_neuronal_dropout %>% evaluate(x_test, y_test)
print(score_dropout)

```

```{r}
library(keras)

# Créer un modèle avec Dropout
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Ajouter un callback pour l'arrêt précoce
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 3,
  restore_best_weights = TRUE
)

# Entraîner le modèle
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(early_stopping)
)

# Évaluer le modèle
score <- model %>% evaluate(x_test, y_test)
print(score)

```

```{r}
library(keras)

# Créer un modèle avec Dropout
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(x_train))) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compiler le modèle
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

# Ajouter un callback pour l'arrêt précoce
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 3,
  restore_best_weights = TRUE
)

# Entraîner le modèle
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(early_stopping)
)

# Évaluer le modèle
score <- model %>% evaluate(x_test, y_test)
print(score)

```


